{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "\n",
    "Here we will dig deeper into the BERT embeddings and their valorization thanks to the best clustering technics. \n",
    "\n",
    "Then we will build our rule based Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 22:59:49.203555: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-15 22:59:49.206511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:59:49.206524: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/qan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import langid, re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "eng_corpus = set(nltk.corpus.words.words())\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201947</th>\n",
       "      <td>@172376 We know staying connected is important, why not take your office to 35k feet? We're so glad you're enjoying the WiFi!</td>\n",
       "      <td>@AmericanAir and @172 have nailed in the transatlantic WiFi service. I am able to join my @172377 daily scrum onboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203418</th>\n",
       "      <td>@172677 We've capped our fares for nonstop flights at $99 for Puerto Rico through the 8th of Oct. Book travel here: https://t.co/iJWiiSmxCO</td>\n",
       "      <td>@AmericanAir Average price of ticket out: $2500 one way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203504</th>\n",
       "      <td>@143005 Please give our Baggage team a call at 800-866-4010 for updates that may be available.</td>\n",
       "      <td>@AmericanAir Really annoyed been over a month since my damaged bag claim never heard back! done as told...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203506</th>\n",
       "      <td>@143005 Our apologies for the hold. Our Central Baggage team will help at that number. If we can get an update, then please DM your bag file number.</td>\n",
       "      <td>@AmericanAir terrible service wait ages trying to call that number almost two months gone no response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203633</th>\n",
       "      <td>@172730 We're providing waivers for St Croix, Gillian. Please take a look at our Travel Alert for STX here: https://t.co/kNkPUiCR67.</td>\n",
       "      <td>@AmericanAir charges their patrons to change their flights every time an airport closes #stcroix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    responce  \\\n",
       "201947                         @172376 We know staying connected is important, why not take your office to 35k feet? We're so glad you're enjoying the WiFi!   \n",
       "203418           @172677 We've capped our fares for nonstop flights at $99 for Puerto Rico through the 8th of Oct. Book travel here: https://t.co/iJWiiSmxCO   \n",
       "203504                                                        @143005 Please give our Baggage team a call at 800-866-4010 for updates that may be available.   \n",
       "203506  @143005 Our apologies for the hold. Our Central Baggage team will help at that number. If we can get an update, then please DM your bag file number.   \n",
       "203633                  @172730 We're providing waivers for St Croix, Gillian. Please take a look at our Travel Alert for STX here: https://t.co/kNkPUiCR67.   \n",
       "\n",
       "                                                                                                                     question  \n",
       "201947  @AmericanAir and @172 have nailed in the transatlantic WiFi service. I am able to join my @172377 daily scrum onboard  \n",
       "203418                                                               @AmericanAir Average price of ticket out: $2500 one way.  \n",
       "203504             @AmericanAir Really annoyed been over a month since my damaged bag claim never heard back! done as told...  \n",
       "203506                  @AmericanAir terrible service wait ages trying to call that number almost two months gone no response  \n",
       "203633                       @AmericanAir charges their patrons to change their flights every time an airport closes #stcroix  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('question_responce.csv', index_col='Unnamed: 0')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have tried to build additional data for the greetings part of the rule based chatbot. It didn't work well do to this, the greetings cluster we had here got drown in the rest of the dataset. Instead we will try to turn this into a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bringing the knew clusters with the original ones doens't work so well \n",
    "# data_suppl = pd.read_csv('question_responce suppl.csv', index_col='Unnamed: 0')[['responce', 'question']]\n",
    "# data_suppl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.append(data_suppl, ignore_index=True)\n",
    "# data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing steps \n",
    "\n",
    "(Detailled explanations in previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove url  \n",
    "def clean_url(df):\n",
    "    tag_url= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    df=tag_url.sub(r'',df)\n",
    "    return df\n",
    "\n",
    "#Remove html link \n",
    "def clean_html(df):\n",
    "    tag_html=re.compile(r'<.*?>')\n",
    "    df=tag_html.sub(r'',df)\n",
    "    return df\n",
    "\n",
    "#Remove all the most recent emojis\n",
    "def remove_emoji(df):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"   \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "                               \n",
    "    return emoji_pattern.sub(r'', df)\n",
    "\n",
    "\n",
    "def clean_punctuation(df):\n",
    "    tag_punct=re.compile(r'[^\\w\\s]')\n",
    "    df=tag_punct.sub(r'',df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_english(df):\n",
    "    ''' Return True if the sentence is in english False otherwise'''\n",
    "    return 1 if langid.classify(df)[0] == 'en' else 0\n",
    "\n",
    "\n",
    "def remove_stops(df):\n",
    "    custom_stopwords = set(stopwords.words(\"english\") + ['amp', 'aa', 'lax', 'flight', 'flying', 'plane', 'flights', 'fly', 'american', 'airlines', 'american_airlines'])\n",
    "    return ' '.join([t for t in word_tokenize(df) if not t in custom_stopwords])\n",
    " \n",
    "\n",
    "def remove_non_english(df): \n",
    "    only_english = \" \".join(w for w in nltk.wordpunct_tokenize(df) \\\n",
    "             if w.lower() in eng_corpus)\n",
    "    return only_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1852, 3)\n",
      "(1818, 3)\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocess (the order is very important here)\n",
    "data['question']=data['question'].apply(lambda x: re.sub('@[\\w]+','',x)) #delate @\n",
    "data['question']=data['question'].apply(lambda x: re.sub('#[^\\s]+','',x)) #delate hastag\n",
    "\n",
    "data['question']=data['question'].apply(lambda x: clean_url(x))\n",
    "data['question']=data['question'].apply(lambda x: clean_html(x))\n",
    "data['question']=data['question'].apply(lambda x: remove_emoji(x))\n",
    "# data['question']=data['question'].apply(lambda x: clean_punctuation(x))\n",
    " \n",
    "# marks english questions with 1 else 0\n",
    "data['langue']=data['question'].apply(lambda x: get_english(x)) \n",
    "print(data.shape)\n",
    "data = data.query('langue == 1')\n",
    "print(data.shape)\n",
    "\n",
    "data['question']=data['question'].apply(lambda x: remove_stops(x.lower()))\n",
    "# data['question']=data['question'].apply(lambda x: remove_non_english(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1818, 3)\n",
      "(1806, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>could someone team available guide gate asap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>ben tennyson pilot . …</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>right , earned . also ’ pay pass spouse . need change program .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         responce  \\\n",
       "603                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "605  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "608                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "612                                                                                     @115906 We're sorry for your frustration.   \n",
       "618                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                            question  langue  \n",
       "603                                 erica team amazing give raise ty       1  \n",
       "605                     could someone team available guide gate asap       1  \n",
       "608                                           ben tennyson pilot . …       1  \n",
       "612  right , earned . also ’ pay pass spouse . need change program .       1  \n",
       "618                     thank , playing great attendants back home !       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "# remove empty question\n",
    "data.question = data.question.apply(lambda x : x.lower().strip())\n",
    "data.drop(data[data.question == \"\"].index, inplace=True)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and clustering using Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 57/57 [00:18<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(data.question.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>could someone team available guide gate asap</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.020876044, 0.03373408, -0.03730834, 0.04945281, -0.0183955, 0.014309846, 0.015990604, -0.010458622, 0.021350361, -0.027525274, 0.030209452, 0.011135467, 0.0014646047, 0.031257655, -0.009953338, -0.06722267, 0.012159301, -0.020916037, -0.0053177583, -0.025816087, 0.013643312, 0.05238593, -0.061271194, 0.003623677, 0.012688841, -0.010534255, 0.0045564706, 0.01598113, 0.033383198, 0.016381817, -0.0010483678, -0.02527402, -0.02857663, -0.024139551, 1.6183155e-06, -0.0013259071, -0.010000101, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>ben tennyson pilot . …</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.008929651, 0.06484396, 0.032495033, -0.009288641, 0.008502924, 0.0039460524, 0.027906708, 0.0136236455, -0.06856497, -0.021749394, 0.026914258, -0.017214887, -0.012272861, 0.013218348, 0.023117177, 0.0054314057, 0.0017828323, -0.0020100235, 0.062044077, -0.010390789, -0.021098925, 0.015394811, -0.043410074, 0.016768014, 0.027112238, -0.023458108, 0.09346981, 0.011450346, 0.029201673, -0.00015037149, -0.015208454, 0.014709363, 0.042184323, 0.0401348, 1.8487254e-06, 0.06487959, 0.041352745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>right , earned . also ’ pay pass spouse . need change program .</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03217489, 0.08505509, -0.011697191, 0.039517846, -0.015567623, 0.03125259, -0.01293914, 0.05513484, 0.013048691, -0.028603366, 0.06413592, 0.04027265, 0.027461449, 0.02202583, 0.010803952, 0.092469916, -0.055953097, 0.012029881, 0.045438394, -0.041170355, -0.0186616, 0.028493868, -0.054801907, 0.03076643, -0.010311727, -0.046284523, 0.052611567, 0.019676004, 0.042133853, 0.020716945, 0.07481377, 0.056581125, -0.058552947, -0.0016325366, 1.7709614e-06, -0.0018588227, -0.008963148, 0.00337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.033029847, -0.052723926, -0.020886198, 0.01134514, 0.014481701, -0.014762324, 0.0074800747, 0.0005944651, -0.030104227, -0.016419565, -0.010901938, -0.038568933, -0.0146933915, 0.03135345, 0.058903363, -0.014872131, 0.0047037266, -0.014426683, -0.043511484, -0.023981038, 0.0020147434, -0.00075374596, -0.01026135, -0.02827919, -0.032739338, 0.07303833, 0.031102788, 0.022675911, 0.030187495, -0.03584585, 0.07961837, -0.027167307, 0.011765745, 0.032604683, 1.8933075e-06, -0.0016266234, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         responce  \\\n",
       "603                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "605  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "608                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "612                                                                                     @115906 We're sorry for your frustration.   \n",
       "618                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                            question  langue  \\\n",
       "603                                 erica team amazing give raise ty       1   \n",
       "605                     could someone team available guide gate asap       1   \n",
       "608                                           ben tennyson pilot . …       1   \n",
       "612  right , earned . also ’ pay pass spouse . need change program .       1   \n",
       "618                     thank , playing great attendants back home !       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            bert_vectors  \n",
       "603  [-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...  \n",
       "605  [0.020876044, 0.03373408, -0.03730834, 0.04945281, -0.0183955, 0.014309846, 0.015990604, -0.010458622, 0.021350361, -0.027525274, 0.030209452, 0.011135467, 0.0014646047, 0.031257655, -0.009953338, -0.06722267, 0.012159301, -0.020916037, -0.0053177583, -0.025816087, 0.013643312, 0.05238593, -0.061271194, 0.003623677, 0.012688841, -0.010534255, 0.0045564706, 0.01598113, 0.033383198, 0.016381817, -0.0010483678, -0.02527402, -0.02857663, -0.024139551, 1.6183155e-06, -0.0013259071, -0.010000101, ...  \n",
       "608  [-0.008929651, 0.06484396, 0.032495033, -0.009288641, 0.008502924, 0.0039460524, 0.027906708, 0.0136236455, -0.06856497, -0.021749394, 0.026914258, -0.017214887, -0.012272861, 0.013218348, 0.023117177, 0.0054314057, 0.0017828323, -0.0020100235, 0.062044077, -0.010390789, -0.021098925, 0.015394811, -0.043410074, 0.016768014, 0.027112238, -0.023458108, 0.09346981, 0.011450346, 0.029201673, -0.00015037149, -0.015208454, 0.014709363, 0.042184323, 0.0401348, 1.8487254e-06, 0.06487959, 0.041352745...  \n",
       "612  [-0.03217489, 0.08505509, -0.011697191, 0.039517846, -0.015567623, 0.03125259, -0.01293914, 0.05513484, 0.013048691, -0.028603366, 0.06413592, 0.04027265, 0.027461449, 0.02202583, 0.010803952, 0.092469916, -0.055953097, 0.012029881, 0.045438394, -0.041170355, -0.0186616, 0.028493868, -0.054801907, 0.03076643, -0.010311727, -0.046284523, 0.052611567, 0.019676004, 0.042133853, 0.020716945, 0.07481377, 0.056581125, -0.058552947, -0.0016325366, 1.7709614e-06, -0.0018588227, -0.008963148, 0.00337...  \n",
       "618  [-0.033029847, -0.052723926, -0.020886198, 0.01134514, 0.014481701, -0.014762324, 0.0074800747, 0.0005944651, -0.030104227, -0.016419565, -0.010901938, -0.038568933, -0.0146933915, 0.03135345, 0.058903363, -0.014872131, 0.0047037266, -0.014426683, -0.043511484, -0.023981038, 0.0020147434, -0.00075374596, -0.01026135, -0.02827919, -0.032739338, 0.07303833, 0.031102788, 0.022675911, 0.030187495, -0.03584585, 0.07961837, -0.027167307, 0.011765745, 0.032604683, 1.8933075e-06, -0.0016266234, -0.0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['bert_vectors'] = list(embeddings)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering steps\n",
    "\n",
    "Here we will try more clustering techniques and try to optimize them the best way possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Kmeans \n",
    "\n",
    "After thinking about the project and talking with the American Airlines teams, we choose to use a bigger cluster number than before.\n",
    "\n",
    "Here we use the Inertia to apply the elbow method and try to find the right cluster number. Silhouette score is also a popular metric in unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -1298.0926513671875\n",
      "Silhouette scores: 0.030501214787364006\n",
      "Inertia: -1282.0140380859375\n",
      "Silhouette scores: 0.029432838782668114\n",
      "Inertia: -1266.9273681640625\n",
      "Silhouette scores: 0.031597722321748734\n",
      "Inertia: -1256.930419921875\n",
      "Silhouette scores: 0.030978398397564888\n",
      "Inertia: -1245.746826171875\n",
      "Silhouette scores: 0.027881905436515808\n",
      "Inertia: -1233.1790771484375\n",
      "Silhouette scores: 0.030981333926320076\n",
      "Inertia: -1222.5321044921875\n",
      "Silhouette scores: 0.03152916207909584\n",
      "Inertia: -1217.146484375\n",
      "Silhouette scores: 0.03085470199584961\n",
      "Inertia: -1207.2088623046875\n",
      "Silhouette scores: 0.030987467616796494\n",
      "Inertia: -1199.92333984375\n",
      "Silhouette scores: 0.03128298744559288\n",
      "Inertia: -1193.8924560546875\n",
      "Silhouette scores: 0.030648531392216682\n",
      "Inertia: -1183.953857421875\n",
      "Silhouette scores: 0.034387800842523575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "losses = []\n",
    "silhouette_scores = []\n",
    "for cluster in np.arange(15,50, 3):\n",
    "    kmeans = KMeans(n_clusters = cluster, random_state=1)\n",
    "    kmeans.fit(embeddings)\n",
    "    print(f'Inertia: {kmeans.score(embeddings)}')\n",
    "    losses.append(kmeans.score(embeddings))\n",
    "    print(f'Silhouette scores: {silhouette_score(embeddings, kmeans.labels_)}')\n",
    "    silhouette_scores.append(silhouette_score(embeddings, kmeans.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(15,50)\n",
    "print('Inertia:')\n",
    "plt.plot(x, losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Silhouette score:')\n",
    "plt.plot(x, silhouette_scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 20 & 47 clusters seems interesting numbers to further explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -1192.5897216796875\n",
      "Silhouette scores: 0.016546668484807014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 20, random_state=1)\n",
    "kmeans.fit(embeddings)\n",
    "print(f'Inertia: {kmeans.score(embeddings)}')\n",
    "print(f'Silhouette scores: {silhouette_score(embeddings, kmeans.labels_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -1112.76708984375\n",
      "Silhouette scores: 0.0175517238676548\n"
     ]
    }
   ],
   "source": [
    "kmeans_47 = KMeans(n_clusters = 47, random_state=1)\n",
    "kmeans_47.fit(embeddings)\n",
    "print(f'Inertia: {kmeans_47.score(embeddings)}')\n",
    "print(f'Silhouette scores: {silhouette_score(embeddings, kmeans_47.labels_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But after discusion with the business team of American Airlines (aka our NLP teacher). We decided to go for 35 clusters. Indeed the metrics for unsupervised learning are really not perfect, so business knowledge will most of the time be more insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -1215.0968017578125\n",
      "Silhouette scores: 0.021248331293463707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans_35 = KMeans(n_clusters = 35, random_state=1)\n",
    "kmeans_35.fit(embeddings)\n",
    "print(f'Inertia: {kmeans_35.score(embeddings)}')\n",
    "print(f'Silhouette scores: {silhouette_score(embeddings, kmeans_35.labels_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>could someone team available guide gate asap</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.020876044, 0.03373408, -0.03730834, 0.04945281, -0.0183955, 0.014309846, 0.015990604, -0.010458622, 0.021350361, -0.027525274, 0.030209452, 0.011135467, 0.0014646047, 0.031257655, -0.009953338, -0.06722267, 0.012159301, -0.020916037, -0.0053177583, -0.025816087, 0.013643312, 0.05238593, -0.061271194, 0.003623677, 0.012688841, -0.010534255, 0.0045564706, 0.01598113, 0.033383198, 0.016381817, -0.0010483678, -0.02527402, -0.02857663, -0.024139551, 1.6183155e-06, -0.0013259071, -0.010000101, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>ben tennyson pilot . …</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.008929651, 0.06484396, 0.032495033, -0.009288641, 0.008502924, 0.0039460524, 0.027906708, 0.0136236455, -0.06856497, -0.021749394, 0.026914258, -0.017214887, -0.012272861, 0.013218348, 0.023117177, 0.0054314057, 0.0017828323, -0.0020100235, 0.062044077, -0.010390789, -0.021098925, 0.015394811, -0.043410074, 0.016768014, 0.027112238, -0.023458108, 0.09346981, 0.011450346, 0.029201673, -0.00015037149, -0.015208454, 0.014709363, 0.042184323, 0.0401348, 1.8487254e-06, 0.06487959, 0.041352745...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>right , earned . also ’ pay pass spouse . need change program .</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03217489, 0.08505509, -0.011697191, 0.039517846, -0.015567623, 0.03125259, -0.01293914, 0.05513484, 0.013048691, -0.028603366, 0.06413592, 0.04027265, 0.027461449, 0.02202583, 0.010803952, 0.092469916, -0.055953097, 0.012029881, 0.045438394, -0.041170355, -0.0186616, 0.028493868, -0.054801907, 0.03076643, -0.010311727, -0.046284523, 0.052611567, 0.019676004, 0.042133853, 0.020716945, 0.07481377, 0.056581125, -0.058552947, -0.0016325366, 1.7709614e-06, -0.0018588227, -0.008963148, 0.00337...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.033029847, -0.052723926, -0.020886198, 0.01134514, 0.014481701, -0.014762324, 0.0074800747, 0.0005944651, -0.030104227, -0.016419565, -0.010901938, -0.038568933, -0.0146933915, 0.03135345, 0.058903363, -0.014872131, 0.0047037266, -0.014426683, -0.043511484, -0.023981038, 0.0020147434, -0.00075374596, -0.01026135, -0.02827919, -0.032739338, 0.07303833, 0.031102788, 0.022675911, 0.030187495, -0.03584585, 0.07961837, -0.027167307, 0.011765745, 0.032604683, 1.8933075e-06, -0.0016266234, -0.0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         responce  \\\n",
       "603                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "605  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "608                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "612                                                                                     @115906 We're sorry for your frustration.   \n",
       "618                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                            question  langue  \\\n",
       "603                                 erica team amazing give raise ty       1   \n",
       "605                     could someone team available guide gate asap       1   \n",
       "608                                           ben tennyson pilot . …       1   \n",
       "612  right , earned . also ’ pay pass spouse . need change program .       1   \n",
       "618                     thank , playing great attendants back home !       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            bert_vectors  \\\n",
       "603  [-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...   \n",
       "605  [0.020876044, 0.03373408, -0.03730834, 0.04945281, -0.0183955, 0.014309846, 0.015990604, -0.010458622, 0.021350361, -0.027525274, 0.030209452, 0.011135467, 0.0014646047, 0.031257655, -0.009953338, -0.06722267, 0.012159301, -0.020916037, -0.0053177583, -0.025816087, 0.013643312, 0.05238593, -0.061271194, 0.003623677, 0.012688841, -0.010534255, 0.0045564706, 0.01598113, 0.033383198, 0.016381817, -0.0010483678, -0.02527402, -0.02857663, -0.024139551, 1.6183155e-06, -0.0013259071, -0.010000101, ...   \n",
       "608  [-0.008929651, 0.06484396, 0.032495033, -0.009288641, 0.008502924, 0.0039460524, 0.027906708, 0.0136236455, -0.06856497, -0.021749394, 0.026914258, -0.017214887, -0.012272861, 0.013218348, 0.023117177, 0.0054314057, 0.0017828323, -0.0020100235, 0.062044077, -0.010390789, -0.021098925, 0.015394811, -0.043410074, 0.016768014, 0.027112238, -0.023458108, 0.09346981, 0.011450346, 0.029201673, -0.00015037149, -0.015208454, 0.014709363, 0.042184323, 0.0401348, 1.8487254e-06, 0.06487959, 0.041352745...   \n",
       "612  [-0.03217489, 0.08505509, -0.011697191, 0.039517846, -0.015567623, 0.03125259, -0.01293914, 0.05513484, 0.013048691, -0.028603366, 0.06413592, 0.04027265, 0.027461449, 0.02202583, 0.010803952, 0.092469916, -0.055953097, 0.012029881, 0.045438394, -0.041170355, -0.0186616, 0.028493868, -0.054801907, 0.03076643, -0.010311727, -0.046284523, 0.052611567, 0.019676004, 0.042133853, 0.020716945, 0.07481377, 0.056581125, -0.058552947, -0.0016325366, 1.7709614e-06, -0.0018588227, -0.008963148, 0.00337...   \n",
       "618  [-0.033029847, -0.052723926, -0.020886198, 0.01134514, 0.014481701, -0.014762324, 0.0074800747, 0.0005944651, -0.030104227, -0.016419565, -0.010901938, -0.038568933, -0.0146933915, 0.03135345, 0.058903363, -0.014872131, 0.0047037266, -0.014426683, -0.043511484, -0.023981038, 0.0020147434, -0.00075374596, -0.01026135, -0.02827919, -0.032739338, 0.07303833, 0.031102788, 0.022675911, 0.030187495, -0.03584585, 0.07961837, -0.027167307, 0.011765745, 0.032604683, 1.8933075e-06, -0.0016266234, -0.0...   \n",
       "\n",
       "     bert_kmeans_35clusters  \n",
       "603                       3  \n",
       "605                       5  \n",
       "608                      34  \n",
       "612                      18  \n",
       "618                       3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['bert_kmeans_clusters'] = kmeans.labels_\n",
    "# data['bert_kmeans_47clusters'] = kmeans_47.labels_\n",
    "data['bert_kmeans_35clusters'] = kmeans_35.labels_\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33    92\n",
       "10    84\n",
       "26    79\n",
       "8     79\n",
       "7     75\n",
       "6     72\n",
       "12    68\n",
       "4     64\n",
       "29    61\n",
       "21    60\n",
       "25    59\n",
       "2     58\n",
       "5     58\n",
       "3     58\n",
       "28    57\n",
       "15    53\n",
       "11    51\n",
       "23    51\n",
       "24    51\n",
       "1     50\n",
       "22    49\n",
       "13    47\n",
       "31    45\n",
       "16    43\n",
       "34    43\n",
       "30    39\n",
       "14    38\n",
       "18    36\n",
       "9     36\n",
       "0     32\n",
       "19    30\n",
       "27    29\n",
       "20    27\n",
       "32    21\n",
       "17    11\n",
       "Name: bert_kmeans_35clusters, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bert_kmeans_35clusters.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics looks more or less balanced with Kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving topics and model\n",
    "data.to_csv('k_means_clusters.csv')\n",
    "pickle.dump(kmeans_35, open('kmeans_35.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With HDBSCAN \n",
    "\n",
    "This algorithm is density based meaning that it might find different clusters than Kmeans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# dbscan_grid = DBSCAN(eps = 0.5, min_samples = 5, n_jobs=12)\n",
    "# clusters_db = dbscan_grid.fit_predict(data.bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "def get_clusters(embeddings,\n",
    "                      n_neighbors,\n",
    "                      n_components, \n",
    "                      min_cluster_size):\n",
    " \n",
    "    umap_emb = (umap.UMAP(n_neighbors=n_neighbors, n_components=n_components, \n",
    "                                metric='cosine', random_state=1)\n",
    "                            .fit_transform(embeddings))\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, metric='euclidean', \n",
    "                               cluster_selection_method='eom').fit(umap_emb)\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, 43, 37, ..., 26, 21, -1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweak parameters\n",
    "clusters = get_clusters(embeddings, 15, 5, 5)\n",
    "clusters.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1     836\n",
       " 15    129\n",
       " 38     95\n",
       " 23     87\n",
       " 12     64\n",
       " 24     51\n",
       " 7      45\n",
       " 29     41\n",
       " 33     34\n",
       " 10     29\n",
       " 11     28\n",
       " 28     26\n",
       " 18     23\n",
       " 4      23\n",
       " 14     19\n",
       " 5      19\n",
       " 31     17\n",
       " 36     17\n",
       " 20     17\n",
       " 8      16\n",
       " 35     13\n",
       " 9      13\n",
       " 21     13\n",
       " 0      13\n",
       " 34     12\n",
       " 30     12\n",
       " 25     10\n",
       " 19     10\n",
       " 2       9\n",
       " 37      9\n",
       " 6       9\n",
       " 26      9\n",
       " 16      8\n",
       " 32      8\n",
       " 13      8\n",
       " 22      7\n",
       " 3       7\n",
       " 17      7\n",
       " 1       7\n",
       " 27      6\n",
       "Name: bert_dbscan_clusters, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_dbscan_clusters\"] = clusters.labels_\n",
    "print(clusters.labels_.max())\n",
    "data.bert_dbscan_clusters.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is also used for anomaly detection and here it finds far too much anomalies (cluster = -1). It's not a good clustering aglorithm for our embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using similarity matrix (based on cosine similarity) and hierarchical clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "# build similarity matrix \n",
    "cos_sim_matrix = hierarchy.linkage(embeddings, \"average\", metric=\"cosine\")\n",
    "# cos_sim_matrix = pairwise_distances(umap_embeddings, metric='cosine', n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweak threshold\n",
    "threshold = 0.75\n",
    "clusters = hierarchy.fcluster(cos_sim_matrix, threshold, criterion=\"distance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125    312\n",
       "128    311\n",
       "127    155\n",
       "95      98\n",
       "131     67\n",
       "75      34\n",
       "106     30\n",
       "118     25\n",
       "115     23\n",
       "113     23\n",
       "76      22\n",
       "92      22\n",
       "126     20\n",
       "105     19\n",
       "94      18\n",
       "104     17\n",
       "112     17\n",
       "117     17\n",
       "77      16\n",
       "82      16\n",
       "55      14\n",
       "129     13\n",
       "79      12\n",
       "87      12\n",
       "88      12\n",
       "110     12\n",
       "124     11\n",
       "81      11\n",
       "108     11\n",
       "72      10\n",
       "85      10\n",
       "130     10\n",
       "63       9\n",
       "102      9\n",
       "59       9\n",
       "37       8\n",
       "100      8\n",
       "50       8\n",
       "123      8\n",
       "68       8\n",
       "54       7\n",
       "114      7\n",
       "43       7\n",
       "120      7\n",
       "74       6\n",
       "116      6\n",
       "93       6\n",
       "44       6\n",
       "62       6\n",
       "122      6\n",
       "15       5\n",
       "52       5\n",
       "73       5\n",
       "19       5\n",
       "96       5\n",
       "49       5\n",
       "83       5\n",
       "6        5\n",
       "80       5\n",
       "141      5\n",
       "45       5\n",
       "103      5\n",
       "70       5\n",
       "9        4\n",
       "121      4\n",
       "101      4\n",
       "154      4\n",
       "111      4\n",
       "151      4\n",
       "119      4\n",
       "109      4\n",
       "138      4\n",
       "160      4\n",
       "3        3\n",
       "56       3\n",
       "140      3\n",
       "150      3\n",
       "4        3\n",
       "149      3\n",
       "47       3\n",
       "90       3\n",
       "153      3\n",
       "64       3\n",
       "69       3\n",
       "99       3\n",
       "147      3\n",
       "78       3\n",
       "42       3\n",
       "107      3\n",
       "33       3\n",
       "91       3\n",
       "51       3\n",
       "13       2\n",
       "145      2\n",
       "146      2\n",
       "35       2\n",
       "142      2\n",
       "46       2\n",
       "135      2\n",
       "60       2\n",
       "11       2\n",
       "34       2\n",
       "148      2\n",
       "159      2\n",
       "21       2\n",
       "139      2\n",
       "137      2\n",
       "20       2\n",
       "28       2\n",
       "1        2\n",
       "144      2\n",
       "18       2\n",
       "161      2\n",
       "2        2\n",
       "29       2\n",
       "40       2\n",
       "86       2\n",
       "58       2\n",
       "157      2\n",
       "97       2\n",
       "53       2\n",
       "65       2\n",
       "23       2\n",
       "98       2\n",
       "67       2\n",
       "143      2\n",
       "38       2\n",
       "8        2\n",
       "132      2\n",
       "89       2\n",
       "71       2\n",
       "158      2\n",
       "162      2\n",
       "41       2\n",
       "27       2\n",
       "39       1\n",
       "26       1\n",
       "24       1\n",
       "12       1\n",
       "30       1\n",
       "14       1\n",
       "57       1\n",
       "16       1\n",
       "156      1\n",
       "136      1\n",
       "61       1\n",
       "22       1\n",
       "17       1\n",
       "5        1\n",
       "31       1\n",
       "25       1\n",
       "32       1\n",
       "36       1\n",
       "66       1\n",
       "133      1\n",
       "84       1\n",
       "10       1\n",
       "7        1\n",
       "48       1\n",
       "134      1\n",
       "155      1\n",
       "152      1\n",
       "Name: bert_hierarchical_clusters, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_hierarchical_clusters\"] = clusters\n",
    "print(data[\"bert_hierarchical_clusters\"].nunique())\n",
    "data[\"bert_hierarchical_clusters\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for DBSCAN, clusters are really unbalanced, we will stick with Kmeans for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intents extraction\n",
    "\n",
    "A basic approach using spacy to get an idea about our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>[-4.28304523e-02  3.36453062e-03 -7.62816006e-03  1.42125487e-02\\n  6.83724955e-02  6.01319745e-02 -6.10870682e-02  2.32692417e-02\\n -5.45795336e-02 -2.88054906e-02  2.58033704e-02  4.80552800e-02\\n -3.95677388e-02  7.16238394e-02  2.56059878e-02 -1.50299408e-02\\n -3.60109173e-02  7.75217786e-02 -2.81775370e-03 -1.30765112e-02\\n  5.26675172e-02  2.61628977e-03  1.58085078e-02  1.38863074e-02\\n -3.71872783e-02  1.85792278e-02 -1.46731446e-02  7.43850842e-02\\n -2.42942888e-02 -3.18094306e-02 -...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         responce  \\\n",
       "603  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "\n",
       "                             question  langue  \\\n",
       "603  erica team amazing give raise ty       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            bert_vectors  \\\n",
       "603  [-4.28304523e-02  3.36453062e-03 -7.62816006e-03  1.42125487e-02\\n  6.83724955e-02  6.01319745e-02 -6.10870682e-02  2.32692417e-02\\n -5.45795336e-02 -2.88054906e-02  2.58033704e-02  4.80552800e-02\\n -3.95677388e-02  7.16238394e-02  2.56059878e-02 -1.50299408e-02\\n -3.60109173e-02  7.75217786e-02 -2.81775370e-03 -1.30765112e-02\\n  5.26675172e-02  2.61628977e-03  1.58085078e-02  1.38863074e-02\\n -3.71872783e-02  1.85792278e-02 -1.46731446e-02  7.43850842e-02\\n -2.42942888e-02 -3.18094306e-02 -...   \n",
       "\n",
       "     bert_kmeans_35clusters  \n",
       "603                       3  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kmeans = pd.read_csv('k_means_clusters.csv', index_col='Unnamed: 0')\n",
    "kmeans_35 = pickle.load(open(\"kmeans_35.sav\", 'rb'))\n",
    "data_kmeans.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def most_common(lst, n_words):\n",
    "    \n",
    "    counter=collections.Counter(lst)\n",
    "    return counter.most_common(n_words)\n",
    "\n",
    "def extract_labels(list_docs):\n",
    "\n",
    "    verbs = []\n",
    "    dobjs = []\n",
    "    nouns = []\n",
    "    adjs = []\n",
    "    \n",
    "    verb = ''\n",
    "    dobj = ''\n",
    "    noun1 = ''\n",
    "    noun2 = ''\n",
    "\n",
    "    # for each document, append verbs, dobs, nouns, and adjectives to \n",
    "    # running lists for whole cluster\n",
    "    for i in range(len(list_docs)):\n",
    "        doc = nlp(list_docs[i])\n",
    "        for token in doc:\n",
    "            if token.is_stop==False:\n",
    "                if token.dep_ == 'ROOT':\n",
    "                    verbs.append(token.text.lower())\n",
    "                elif token.dep_=='dobj':\n",
    "                    dobjs.append(token.lemma_.lower())\n",
    "                elif token.pos_=='NOUN':\n",
    "                    nouns.append(token.lemma_.lower())     \n",
    "                elif token.pos_=='ADJ':\n",
    "                    adjs.append(token.lemma_.lower())\n",
    "    \n",
    "    # take most common words of each kind\n",
    "    if len(verbs) > 0:\n",
    "        verb = most_common(verbs, 1)[0][0]\n",
    "    if len(dobjs) > 0:\n",
    "        dobj = most_common(dobjs, 1)[0][0]\n",
    "    if len(nouns) > 0:\n",
    "        noun1 = most_common(nouns, 1)[0][0]\n",
    "    if len(set(nouns)) > 1:\n",
    "        noun2 = most_common(nouns, 2)[1][0]\n",
    "    \n",
    "    # concatenate the most common words of each kind\n",
    "    label_words = [verb, dobj]\n",
    "    \n",
    "    for word in [noun1, noun2]:\n",
    "        if word not in label_words:\n",
    "            label_words.append(word)\n",
    "\n",
    "    if '' in label_words:\n",
    "        label_words.remove('')\n",
    "    \n",
    "    label = '_'.join(label_words)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['question'] = data['question'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "intents = {}\n",
    "for cluster in data['bert_kmeans_35clusters'].unique().tolist():\n",
    "    intents['cluster_' + str(cluster)] = extract_labels(data[data['bert_kmeans_35clusters']==cluster]['question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_18': 'got_raise_flight_attendant',\n",
       " 'cluster_23': 'waiting_flight_gate_airport',\n",
       " 'cluster_30': 'want_plane_flight',\n",
       " 'cluster_1': 'thanks_mile_ticket_way',\n",
       " 'cluster_31': 'thank_flight_service',\n",
       " 'cluster_24': 'fly_wifi_airline_flight',\n",
       " 'cluster_12': 'like_lounge_flagship',\n",
       " 'cluster_14': 'tried_app_flight',\n",
       " 'cluster_33': 'need_flight_ticket',\n",
       " 'cluster_27': 'said_money_guy_people',\n",
       " 'cluster_22': 'charged_bag_flight',\n",
       " 'cluster_16': 'thanks_seat_row',\n",
       " 'cluster_5': 'waiting_refund_day_ticket',\n",
       " 'cluster_29': 'want_seat_flight',\n",
       " 'cluster_0': 'going_flight_plane',\n",
       " 'cluster_6': 'delayed_flight_hour',\n",
       " 'cluster_8': 'delayed_flight_hour',\n",
       " 'cluster_19': 'check_bag_gate',\n",
       " 'cluster_7': 'flying_flight_day_way',\n",
       " 'cluster_17': 'got_flight_aa',\n",
       " 'cluster_26': 'thank_service_medium_team',\n",
       " 'cluster_15': 'need_character_password',\n",
       " 'cluster_34': 'service_customer',\n",
       " 'cluster_11': 'flying_flight_time',\n",
       " 'cluster_4': 'refused_passenger_flight_class',\n",
       " 'cluster_32': 'fees_bag_carry',\n",
       " 'cluster_3': 'thanks_thanksgiving_guy_day',\n",
       " 'cluster_20': 'thanks_bag_baggage',\n",
       " 'cluster_28': 'going_flight_tonight',\n",
       " 'cluster_9': 'thanks_flight_customer',\n",
       " 'cluster_13': 'sent_dm_record',\n",
       " 'cluster_25': 'got_voucher_flight_food',\n",
       " 'cluster_21': 'thanks_discount_flight_trip',\n",
       " 'cluster_2': 'know_flight_pilot',\n",
       " 'cluster_10': 'aa_upgrade_flight'}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this model work based on statistical properties of a text (such as count) and not so much on semantic similarity. Let's try something a bit more advanced.\n",
    "\n",
    "\n",
    "### Intents extraction with Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# off the shelf, BERT based key word extractor\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patterns(question):\n",
    "    \n",
    "    import operator\n",
    "    \n",
    "    two = kw_model.extract_keywords(question, keyphrase_ngram_range=(1, 2),stop_words=None)\n",
    "    three = kw_model.extract_keywords(question, keyphrase_ngram_range=(1, 3),stop_words=None)\n",
    "    four = kw_model.extract_keywords(question, keyphrase_ngram_range=(1, 4),stop_words=None)\n",
    "\n",
    "    final_patterns = two + three + four\n",
    "    if len(final_patterns) != 0 :\n",
    "        result = max(final_patterns,key=operator.itemgetter(1))[0]\n",
    "    else :\n",
    "        result = ''\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_tags(question):\n",
    "    \n",
    "    import operator\n",
    "    tag = kw_model.extract_keywords(question, keyphrase_ngram_range=(1, 1),stop_words=None)\n",
    "    if len(tag) != 0 :\n",
    "            result = max(tag,key=operator.itemgetter(1))[0]\n",
    "    else :\n",
    "        result = ''\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [01:15<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "intents = {}\n",
    "for cluster in tqdm(data['bert_kmeans_35clusters'].unique().tolist()):\n",
    "    intents['cluster_' + str(cluster)] = create_patterns(re.sub(\"\\d+\", \" \", \" \".join(data[data['bert_kmeans_35clusters']==cluster]['question_'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_10': 'club much great crew',\n",
       " 'cluster_26': 'hours gate gate agents',\n",
       " 'cluster_33': 'pilot pilot smoothest',\n",
       " 'cluster_27': 'service line unfair guys',\n",
       " 'cluster_1': 'attendants always rude flown',\n",
       " 'cluster_16': 'worth membership way trips',\n",
       " 'cluster_28': 'reservation got ticket get',\n",
       " 'cluster_4': 'way paid upgraded seats',\n",
       " 'cluster_6': 'accidentally something account say',\n",
       " 'cluster_31': 'gone airport extends courtesy',\n",
       " 'cluster_32': 'waiting compensate crap put',\n",
       " 'cluster_23': 'theres taxis tips relax',\n",
       " 'cluster_19': 'give grief things go',\n",
       " 'cluster_14': 'carry bag room overhead',\n",
       " 'cluster_18': 'legroom passengers',\n",
       " 'cluster_13': 'crew terminal awful night',\n",
       " 'cluster_22': 'delayed cant find pilots',\n",
       " 'cluster_15': 'airline taking',\n",
       " 'cluster_12': 'family get traveling yo',\n",
       " 'cluster_2': 'bag fee returned bags',\n",
       " 'cluster_9': 'cancellation due pilot shortage',\n",
       " 'cluster_17': 'responsive helpful making bad',\n",
       " 'cluster_8': 'boarding pass except',\n",
       " 'cluster_11': 'thankful excellent service thanksgiving',\n",
       " 'cluster_30': 'get best customer service',\n",
       " 'cluster_21': 'line fast customer service',\n",
       " 'cluster_34': 'delayed baggage paid leave',\n",
       " 'cluster_25': 'job staff get',\n",
       " 'cluster_7': 'morning see look today',\n",
       " 'cluster_0': 'food options',\n",
       " 'cluster_24': 'diverted locator need phone',\n",
       " 'cluster_20': 'offered upgrades like upgrades',\n",
       " 'cluster_5': 'fix going lose customers',\n",
       " 'cluster_29': 'meal vouchers',\n",
       " 'cluster_3': 'thanksgiving delayed pm staff'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the tag of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tags = {}\n",
    "for cluster in tqdm(data['bert_kmeans_35clusters'].unique().tolist()):\n",
    "    tags['cluster_' + str(cluster)] = create_tags(re.sub(\"\\d+\", \" \", \" \".join(data[data['bert_kmeans_35clusters']==cluster]['question'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_10': 'amaze',\n",
       " 'cluster_26': 'gate',\n",
       " 'cluster_33': 'pilot',\n",
       " 'cluster_27': 'attendant',\n",
       " 'cluster_1': 'attendant',\n",
       " 'cluster_16': 'airport',\n",
       " 'cluster_28': 'booking',\n",
       " 'cluster_4': 'surcharge',\n",
       " 'cluster_6': 'ban',\n",
       " 'cluster_31': 'airport',\n",
       " 'cluster_32': 'message',\n",
       " 'cluster_23': 'taxiway',\n",
       " 'cluster_19': 'grief',\n",
       " 'cluster_14': 'baggage',\n",
       " 'cluster_18': 'seating',\n",
       " 'cluster_13': 'crew',\n",
       " 'cluster_22': 'delay',\n",
       " 'cluster_15': 'airline',\n",
       " 'cluster_12': 'travel',\n",
       " 'cluster_2': 'baggage',\n",
       " 'cluster_9': 'cancellation',\n",
       " 'cluster_17': 'feedback',\n",
       " 'cluster_8': 'boarding',\n",
       " 'cluster_11': 'thankful',\n",
       " 'cluster_30': 'service',\n",
       " 'cluster_21': 'customer',\n",
       " 'cluster_34': 'baggage',\n",
       " 'cluster_25': 'staff',\n",
       " 'cluster_7': 'day',\n",
       " 'cluster_0': 'meal',\n",
       " 'cluster_24': 'locator',\n",
       " 'cluster_20': 'upgrade',\n",
       " 'cluster_5': 'reimburse',\n",
       " 'cluster_29': 'voucher',\n",
       " 'cluster_3': 'reschedule'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard too tell, but we think the first method is the best here to understand our clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot (without greetings)\n",
    " \n",
    "Ok now we have extracted our intents we can build the chatbot, this requires several steps:\n",
    "\n",
    "- process the user input\n",
    "- find the corresponding intent\n",
    "- among the questions belonging to this intent, find the closest from the input, using cosine similarity.\n",
    "- process and provide the corresponding answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         responce  \\\n",
       "603  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "\n",
       "                             question  langue  \\\n",
       "603  erica team amazing give raise ty       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            bert_vectors  \\\n",
       "603  [-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...   \n",
       "\n",
       "     bert_kmeans_35clusters  \n",
       "603                       3  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# loading models\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "kmeans_35 = pickle.load(open(\"kmeans_35.sav\", 'rb'))\n",
    "# data_kmeans = pd.read_csv('k_means_clusters.csv', index_col='Unnamed: 0')\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(question):\n",
    " \n",
    "    import re\n",
    "    question = re.sub('@[\\w]+','',question) #delate @\n",
    "    question = re.sub('#[^\\s]+','',question) #delate hastag\n",
    "    question = clean_url(question)\n",
    "    question = clean_html(question)\n",
    "    question = remove_emoji(question)\n",
    "    question = clean_punctuation(question)\n",
    "    \n",
    "    if get_english(question):\n",
    "        question = remove_stops(question.lower())\n",
    "        # question = remove_non_english(question)\n",
    "        \n",
    "        if question == \"\":\n",
    "            print('Sorry, I do not understand!')\n",
    "    else:\n",
    "        ('Please, talk to me in english!')\n",
    " \n",
    "    return question\n",
    "\n",
    "def encode_input(cleaned_question):\n",
    "    embedding = model.encode(cleaned_question, show_progress_bar=True)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def predict_intent(embedding):\n",
    "    intent = kmeans_35.predict(embedding.reshape(1, -1))\n",
    "    return intent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example inputs to illustrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = \"@AmericanAir I am a regular client of your company and I was sitted right next to a woman with a huge dog. And guess what ? I am allergic, how could you allow a 40lb dog to travel among all passengers ? Seriously It's ridiculous...\"\n",
    "test2 = \"@AmericanAir You guys are always late, my flight is reschedule for the third time now... I can't believe this is happening to me again... I can afford to be late at work!\"\n",
    "\n",
    "\n",
    "# cleaning and vectorizing the user input\n",
    "vectorized_query = encode_input(process_input(test1))\n",
    "\n",
    "# predict the intent of it\n",
    "int(predict_intent(vectorized_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the intent, so now we can subset the dataset according to that intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>@116577 Any dogs you see outside of a kennel are either service or emotional support animals.</td>\n",
       "      <td>’ missing point . numerous people tripped dog , mention anyone might allergic . change policy allow size dog cabins any/no reason ?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.019941103, -0.004907608, -0.0216171, -0.005585694, 0.031978812, 0.013888998, 0.06464607, 0.028766025, 0.037617676, -0.0028157907, 0.03133589, 0.033286024, -0.064723894, -0.0029695826, 0.018896658, 0.022669671, 0.0032928328, 0.009101168, -0.04615694, -0.018063793, -0.0043026083, -0.0058758133, -0.0057010655, -0.0017028486, 0.03471668, 0.029610794, -0.015120311, -0.030667445, 0.030693904, -0.09859003, 0.017937621, -0.06099298, 0.056220766, -0.0021184098, 1.4605016e-06, 0.03779077, -0.035800...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>@117169 Oh my, we'd like to have more details about what happened. We always expect our team to treat our customers courteously and friendly.</td>\n",
       "      <td>interacted one rudest attendants 1305 ord slc . patient , 's 4 .</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0193409, -0.040967677, 0.0029913886, 0.043220628, 0.015281247, 0.023483228, 0.043141637, 0.00888277, -0.023995077, 0.030567506, 0.028364982, -0.040634938, -0.002011723, -0.018827053, -0.015872188, 0.028817305, 0.026407236, 0.0081881, -0.020659061, 0.012174327, -0.03381198, 0.013956402, -0.041634835, 0.029872416, -0.008017442, 0.022238312, 0.0136669045, 0.024514796, 0.041725792, -0.020997101, 0.08472193, 0.003538265, -0.012988361, 0.07707981, 1.8862413e-06, -0.014254608, 0.030600594, 0.032...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>@117240 We want to make sure communication is crystal clear. Please DM your record locator so we can share your feedback.</td>\n",
       "      <td>might good idea 2 working intercoms flights.flight attendant literally sounds like adults talking charlie brown</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.014428796, 0.059674997, -0.012037892, 0.0053907107, -0.0016390622, 0.010717492, 0.08590115, 0.008355415, -0.06536237, 0.027348455, -0.023571568, -0.08685961, -0.006141257, -0.017219562, 0.0059268805, 0.00086474646, -0.019501109, -0.042360302, -0.008528918, -0.012924929, -0.017408704, 0.021890573, -0.030365054, 0.011936902, -0.0085192975, 0.03934497, 0.049262054, 0.010332767, 0.031089215, 0.011980173, 0.07001434, 0.02051775, -0.0021518262, 0.04696523, 1.4659521e-06, 0.0387265, 0.028057527,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7266</th>\n",
       "      <td>@117992 No problem at all. It’s a huge honor for us to support our troops.</td>\n",
       "      <td>extends courtesy every every airport . guess spoiled . thank ur patriotism</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.04100923, -0.024521094, -0.007926439, 0.030299803, -0.0070702266, -0.017769411, 0.037915904, -0.03690842, -0.002167035, -0.0069213337, -0.0016884161, -0.011797779, -0.00501809, -0.030108318, 0.0048918766, 0.029450022, 0.013144511, -0.04678468, -0.043816388, -0.0494837, -0.028091177, 0.028538296, -0.014600648, -0.012067497, -0.017550886, 0.066612035, 0.0734596, -0.0058549293, 0.056001555, -0.022256361, 0.05445566, 0.0024471262, 0.038176324, -0.010516581, 1.6540228e-06, -0.023542032, 0.039...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>@118743 Please DM your record locator. We'd like to share this feedback with the Flight Service leadership team.</td>\n",
       "      <td>comment ’ address problematic comment made attendant .</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10750002, -0.024768135, 0.0011373035, 0.07505461, 0.028067058, 0.048994884, 0.06295916, 0.013057833, -0.054938585, -0.050252527, 0.015565306, -0.015696786, 0.0113104405, -0.060148362, 0.031577054, 0.03322621, 0.02599203, 0.012045059, 0.014371287, -0.010610244, -0.0087413695, 0.020397222, -0.012299838, 0.02811512, -0.04806228, 0.02100706, 0.037713185, 0.009778312, 0.028147966, -0.010626361, 0.06832403, -0.0027342278, 0.007258156, -0.024819054, 1.3524857e-06, -0.025773114, 0.029210512, 0.03...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           responce  \\\n",
       "2708                                                  @116577 Any dogs you see outside of a kennel are either service or emotional support animals.   \n",
       "4562  @117169 Oh my, we'd like to have more details about what happened. We always expect our team to treat our customers courteously and friendly.   \n",
       "4791                      @117240 We want to make sure communication is crystal clear. Please DM your record locator so we can share your feedback.   \n",
       "7266                                                                     @117992 No problem at all. It’s a huge honor for us to support our troops.   \n",
       "9544                               @118743 Please DM your record locator. We'd like to share this feedback with the Flight Service leadership team.   \n",
       "\n",
       "                                                                                                                                 question  \\\n",
       "2708  ’ missing point . numerous people tripped dog , mention anyone might allergic . change policy allow size dog cabins any/no reason ?   \n",
       "4562                                                                     interacted one rudest attendants 1305 ord slc . patient , 's 4 .   \n",
       "4791                      might good idea 2 working intercoms flights.flight attendant literally sounds like adults talking charlie brown   \n",
       "7266                                                           extends courtesy every every airport . guess spoiled . thank ur patriotism   \n",
       "9544                                                                               comment ’ address problematic comment made attendant .   \n",
       "\n",
       "      langue  \\\n",
       "2708       1   \n",
       "4562       1   \n",
       "4791       1   \n",
       "7266       1   \n",
       "9544       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             bert_vectors  \\\n",
       "2708  [0.019941103, -0.004907608, -0.0216171, -0.005585694, 0.031978812, 0.013888998, 0.06464607, 0.028766025, 0.037617676, -0.0028157907, 0.03133589, 0.033286024, -0.064723894, -0.0029695826, 0.018896658, 0.022669671, 0.0032928328, 0.009101168, -0.04615694, -0.018063793, -0.0043026083, -0.0058758133, -0.0057010655, -0.0017028486, 0.03471668, 0.029610794, -0.015120311, -0.030667445, 0.030693904, -0.09859003, 0.017937621, -0.06099298, 0.056220766, -0.0021184098, 1.4605016e-06, 0.03779077, -0.035800...   \n",
       "4562  [0.0193409, -0.040967677, 0.0029913886, 0.043220628, 0.015281247, 0.023483228, 0.043141637, 0.00888277, -0.023995077, 0.030567506, 0.028364982, -0.040634938, -0.002011723, -0.018827053, -0.015872188, 0.028817305, 0.026407236, 0.0081881, -0.020659061, 0.012174327, -0.03381198, 0.013956402, -0.041634835, 0.029872416, -0.008017442, 0.022238312, 0.0136669045, 0.024514796, 0.041725792, -0.020997101, 0.08472193, 0.003538265, -0.012988361, 0.07707981, 1.8862413e-06, -0.014254608, 0.030600594, 0.032...   \n",
       "4791  [0.014428796, 0.059674997, -0.012037892, 0.0053907107, -0.0016390622, 0.010717492, 0.08590115, 0.008355415, -0.06536237, 0.027348455, -0.023571568, -0.08685961, -0.006141257, -0.017219562, 0.0059268805, 0.00086474646, -0.019501109, -0.042360302, -0.008528918, -0.012924929, -0.017408704, 0.021890573, -0.030365054, 0.011936902, -0.0085192975, 0.03934497, 0.049262054, 0.010332767, 0.031089215, 0.011980173, 0.07001434, 0.02051775, -0.0021518262, 0.04696523, 1.4659521e-06, 0.0387265, 0.028057527,...   \n",
       "7266  [-0.04100923, -0.024521094, -0.007926439, 0.030299803, -0.0070702266, -0.017769411, 0.037915904, -0.03690842, -0.002167035, -0.0069213337, -0.0016884161, -0.011797779, -0.00501809, -0.030108318, 0.0048918766, 0.029450022, 0.013144511, -0.04678468, -0.043816388, -0.0494837, -0.028091177, 0.028538296, -0.014600648, -0.012067497, -0.017550886, 0.066612035, 0.0734596, -0.0058549293, 0.056001555, -0.022256361, 0.05445566, 0.0024471262, 0.038176324, -0.010516581, 1.6540228e-06, -0.023542032, 0.039...   \n",
       "9544  [0.10750002, -0.024768135, 0.0011373035, 0.07505461, 0.028067058, 0.048994884, 0.06295916, 0.013057833, -0.054938585, -0.050252527, 0.015565306, -0.015696786, 0.0113104405, -0.060148362, 0.031577054, 0.03322621, 0.02599203, 0.012045059, 0.014371287, -0.010610244, -0.0087413695, 0.020397222, -0.012299838, 0.02811512, -0.04806228, 0.02100706, 0.037713185, 0.009778312, 0.028147966, -0.010626361, 0.06832403, -0.0027342278, 0.007258156, -0.024819054, 1.3524857e-06, -0.025773114, 0.029210512, 0.03...   \n",
       "\n",
       "      bert_kmeans_35clusters  \n",
       "2708                       6  \n",
       "4562                       6  \n",
       "4791                       6  \n",
       "7266                       6  \n",
       "9544                       6  "
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_intent = data[data['bert_kmeans_35clusters'] == int(predict_intent(vectorized_query))]\n",
    "subset_intent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, among this subset corresponding to the predicted intent, we can find the closest question according to cosine similarity. Then we get the corresponding answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9110/1443037527.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>@116577 Any dogs you see outside of a kennel are either service or emotional support animals.</td>\n",
       "      <td>’ missing point . numerous people tripped dog , mention anyone might allergic . change policy allow size dog cabins any/no reason ?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.019941103, -0.004907608, -0.0216171, -0.005585694, 0.031978812, 0.013888998, 0.06464607, 0.028766025, 0.037617676, -0.0028157907, 0.03133589, 0.033286024, -0.064723894, -0.0029695826, 0.018896658, 0.022669671, 0.0032928328, 0.009101168, -0.04615694, -0.018063793, -0.0043026083, -0.0058758133, -0.0057010655, -0.0017028486, 0.03471668, 0.029610794, -0.015120311, -0.030667445, 0.030693904, -0.09859003, 0.017937621, -0.06099298, 0.056220766, -0.0021184098, 1.4605016e-06, 0.03779077, -0.035800...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.721346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165806</th>\n",
       "      <td>@162591 Your comments concern us, service animals are welcome on board if they meet requirements. Please DM some additional details.</td>\n",
       "      <td>harassing service dog &amp; ; yelling license , really great way treat disabled passengers</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0120343985, 0.03918638, 0.011316754, -0.013351571, -0.0011051093, 0.043053478, 0.07687392, 0.034477897, -0.0374002, -0.032502696, 0.02587848, 0.05531184, -0.015721345, -0.012173167, -0.00687013, -0.017690364, -0.0025414717, 0.0028585424, 0.023136841, -0.04097612, -0.012675235, 0.014907694, -0.018480884, 0.025580257, -0.031059088, 0.03991676, 0.037321143, 0.015585828, 0.08703588, -0.09751599, 0.0045243157, -0.02829027, 0.014287325, 0.037151564, 1.2090011e-06, 0.010610217, -0.00088258187, 0...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.649292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116843</th>\n",
       "      <td>@148703 We'd like to help, and encourage you to send us a DM with the cargo shipping details on your furry friend.</td>\n",
       "      <td>flew fam home thanksgiving husband deployed cheer kids . 's stressful trip thanks cargo . terrible service &amp; ; staff n't helpful giving accurate info shipping dog .</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.015265258, 0.0024983129, -0.026950223, -0.00930093, -0.002611777, 0.019893788, 0.022034151, 0.03640407, -0.06019559, -0.04497301, 0.017479245, 6.6361215e-05, 0.027734458, 0.050545532, -0.006065772, 0.037056733, 0.050290518, -0.01156415, -0.06745149, -0.03840999, -0.031434063, 0.007949804, -0.0517855, 0.008663543, -0.009983293, 0.0073636584, -0.020533811, 0.053143118, 0.030936623, -0.087912425, 0.062403332, -0.021068614, -0.0020646944, 0.022940213, 2.0149052e-06, 0.003086647, -0.031838402,...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.491193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69561</th>\n",
       "      <td>@135471 We definitely want to keep families flying this holiday season and fully expect to avoid any cancellations.</td>\n",
       "      <td>previous years , complained , one gave poop ! fact yr baby cried till . fussing husband &amp; ; mom , getting help baby almost fell tarmac bridge . attendant threatened throw ! even acting wild .</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0029673816, 0.04226523, -0.012233615, 0.0052738134, -0.0007847944, -0.009925255, -0.019602045, 0.01412271, -0.07342135, -0.045617957, 0.076577686, -0.057490822, -0.0132762045, 0.0035805374, -0.04294686, 0.0031142463, 0.0038870361, -0.01029153, -0.029974723, 0.013130828, -0.011261489, 0.017865794, -0.0035415662, 0.01381836, -0.05548751, 0.020774115, 0.022960411, 0.020851994, 0.0006317582, -0.06534657, 0.07664578, 0.0037343472, 0.045385342, 0.07073427, 1.5672108e-06, 0.015980422, -0.022511...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.481006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90969</th>\n",
       "      <td>@141669 Sharon is truly one of a kind and yet again this goes to prove how #AAmazing she is. We'll be sure to pass your kudos along. #AATeam</td>\n",
       "      <td>. wonderful booking experience - traveling w/ 2 small poodles . 1 emotional support . anxious process , special services rep , sharon , amazing put completely ease .</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.011245241, -0.008530712, -0.024801826, 0.035152458, -0.012648453, 0.013389804, 0.00331213, 0.05716098, -0.07639315, 0.0013229937, -0.02563098, -0.036300056, 0.03582617, 0.0370557, -0.0057717366, -0.004112909, 0.040112063, -0.011310159, -0.04519837, 0.01143345, -0.045678813, 0.013526977, -0.09896839, 0.036567252, 0.028410507, -0.03288119, -0.06453309, 0.0672273, 0.06397794, -0.03202529, 0.0022711062, -0.036448997, 0.0021409937, 0.01093745, 1.604563e-06, 0.022059651, -0.0236274, 0.002364451...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.464649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            responce  \\\n",
       "2708                                                   @116577 Any dogs you see outside of a kennel are either service or emotional support animals.   \n",
       "165806          @162591 Your comments concern us, service animals are welcome on board if they meet requirements. Please DM some additional details.   \n",
       "116843                            @148703 We'd like to help, and encourage you to send us a DM with the cargo shipping details on your furry friend.   \n",
       "69561                            @135471 We definitely want to keep families flying this holiday season and fully expect to avoid any cancellations.   \n",
       "90969   @141669 Sharon is truly one of a kind and yet again this goes to prove how #AAmazing she is. We'll be sure to pass your kudos along. #AATeam   \n",
       "\n",
       "                                                                                                                                                                                               question  \\\n",
       "2708                                                                ’ missing point . numerous people tripped dog , mention anyone might allergic . change policy allow size dog cabins any/no reason ?   \n",
       "165806                                                                                                           harassing service dog & ; yelling license , really great way treat disabled passengers   \n",
       "116843                             flew fam home thanksgiving husband deployed cheer kids . 's stressful trip thanks cargo . terrible service & ; staff n't helpful giving accurate info shipping dog .   \n",
       "69561   previous years , complained , one gave poop ! fact yr baby cried till . fussing husband & ; mom , getting help baby almost fell tarmac bridge . attendant threatened throw ! even acting wild .   \n",
       "90969                             . wonderful booking experience - traveling w/ 2 small poodles . 1 emotional support . anxious process , special services rep , sharon , amazing put completely ease .   \n",
       "\n",
       "        langue  \\\n",
       "2708         1   \n",
       "165806       1   \n",
       "116843       1   \n",
       "69561        1   \n",
       "90969        1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               bert_vectors  \\\n",
       "2708    [0.019941103, -0.004907608, -0.0216171, -0.005585694, 0.031978812, 0.013888998, 0.06464607, 0.028766025, 0.037617676, -0.0028157907, 0.03133589, 0.033286024, -0.064723894, -0.0029695826, 0.018896658, 0.022669671, 0.0032928328, 0.009101168, -0.04615694, -0.018063793, -0.0043026083, -0.0058758133, -0.0057010655, -0.0017028486, 0.03471668, 0.029610794, -0.015120311, -0.030667445, 0.030693904, -0.09859003, 0.017937621, -0.06099298, 0.056220766, -0.0021184098, 1.4605016e-06, 0.03779077, -0.035800...   \n",
       "165806  [0.0120343985, 0.03918638, 0.011316754, -0.013351571, -0.0011051093, 0.043053478, 0.07687392, 0.034477897, -0.0374002, -0.032502696, 0.02587848, 0.05531184, -0.015721345, -0.012173167, -0.00687013, -0.017690364, -0.0025414717, 0.0028585424, 0.023136841, -0.04097612, -0.012675235, 0.014907694, -0.018480884, 0.025580257, -0.031059088, 0.03991676, 0.037321143, 0.015585828, 0.08703588, -0.09751599, 0.0045243157, -0.02829027, 0.014287325, 0.037151564, 1.2090011e-06, 0.010610217, -0.00088258187, 0...   \n",
       "116843  [0.015265258, 0.0024983129, -0.026950223, -0.00930093, -0.002611777, 0.019893788, 0.022034151, 0.03640407, -0.06019559, -0.04497301, 0.017479245, 6.6361215e-05, 0.027734458, 0.050545532, -0.006065772, 0.037056733, 0.050290518, -0.01156415, -0.06745149, -0.03840999, -0.031434063, 0.007949804, -0.0517855, 0.008663543, -0.009983293, 0.0073636584, -0.020533811, 0.053143118, 0.030936623, -0.087912425, 0.062403332, -0.021068614, -0.0020646944, 0.022940213, 2.0149052e-06, 0.003086647, -0.031838402,...   \n",
       "69561   [-0.0029673816, 0.04226523, -0.012233615, 0.0052738134, -0.0007847944, -0.009925255, -0.019602045, 0.01412271, -0.07342135, -0.045617957, 0.076577686, -0.057490822, -0.0132762045, 0.0035805374, -0.04294686, 0.0031142463, 0.0038870361, -0.01029153, -0.029974723, 0.013130828, -0.011261489, 0.017865794, -0.0035415662, 0.01381836, -0.05548751, 0.020774115, 0.022960411, 0.020851994, 0.0006317582, -0.06534657, 0.07664578, 0.0037343472, 0.045385342, 0.07073427, 1.5672108e-06, 0.015980422, -0.022511...   \n",
       "90969   [0.011245241, -0.008530712, -0.024801826, 0.035152458, -0.012648453, 0.013389804, 0.00331213, 0.05716098, -0.07639315, 0.0013229937, -0.02563098, -0.036300056, 0.03582617, 0.0370557, -0.0057717366, -0.004112909, 0.040112063, -0.011310159, -0.04519837, 0.01143345, -0.045678813, 0.013526977, -0.09896839, 0.036567252, 0.028410507, -0.03288119, -0.06453309, 0.0672273, 0.06397794, -0.03202529, 0.0022711062, -0.036448997, 0.0021409937, 0.01093745, 1.604563e-06, 0.022059651, -0.0236274, 0.002364451...   \n",
       "\n",
       "        bert_kmeans_35clusters  cosine_similarity  \n",
       "2708                         6           0.721346  \n",
       "165806                       6           0.649292  \n",
       "116843                       6           0.491193  \n",
       "69561                        6           0.481006  \n",
       "90969                        6           0.464649  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n",
    "subset_intent.sort_values(by=['cosine_similarity'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But know we have to process this anwser : removing the person names, places, prices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_names(df):\n",
    "    tagged_sentence = nltk.tag.pos_tag(df.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP']\n",
    "    df_final = ' '.join(edited_sentence)\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def remove_stops_anwsers(df):\n",
    "    custom_stopwords = set(stopwords.words(\"english\"))\n",
    "    return ' '.join([t for t in df.split()  if not t in custom_stopwords])\n",
    "\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "def english_words(sent):\n",
    "    from textblob import Word\n",
    "    url = re.search(\"(?P<url>https?://[^\\s]+)\", sent)\n",
    "    if url is not None:\n",
    "        url = url.group(0)\n",
    "    return \" \".join(w for w in sent.split() if Word(w.lower()).lemmatize() in words or w == url  or w.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(answer):\n",
    " \n",
    "    import re\n",
    "    answer = re.sub('@[\\w]+','',answer) #delate @\n",
    "    answer = re.sub('#[^\\s]+','',answer) #delate hastag\n",
    "    # remove numbers below 6 figures (to keep phone number in anwsers)\n",
    "    answer = re.sub('(\\A|[^0-9])([0-9]{1,6})([^0-9]|$)', '', answer) \n",
    "    # question = clean_url(question)\n",
    "    # answer = remove_stops_anwsers(anwser)\n",
    "    answer = remove_names(answer)\n",
    "    answer = remove_emoji(answer)\n",
    "    # answer = english_words(answer)\n",
    "    \n",
    "    # question = clean_punctuation(question)\n",
    " \n",
    "    return answer.capitalize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AmericanAir You guys are always late, my flight is reschedule for the third time now... I can't believe this is happening to me again... I can afford to be late at work!\n",
      "@116577 Any dogs you see outside of a kennel are either service or emotional support animals.\n",
      "Any dogs you see outside of a kennel are either service or emotional support animals.\n"
     ]
    }
   ],
   "source": [
    "anwser = subset_intent.sort_values(by=['cosine_similarity'], ascending=False).responce.iat[0]\n",
    "print(test2)\n",
    "print(anwser)\n",
    "print(process_answer(anwser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with greetings \n",
    "\n",
    "We failed to add the greetings synthetic dataset and find relevant cluster with our initial pipeline. This has been adding a lot of noise in the data. So we will try a different approach here.\n",
    "\n",
    "Since we made up this greeting dataset, we already know the intents of it. We are going to make this rule based chatbot a supervised learning tasks : with the intents with found in the american airlines dataset and with the synthetic topics we labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203634</th>\n",
       "      <td>Hi !</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203635</th>\n",
       "      <td>How are you ?</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203636</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>Good to see you, what can i do for you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203637</th>\n",
       "      <td>Hello !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203638</th>\n",
       "      <td>Hey !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                responce                                 question  langue  \\\n",
       "203634              Hi !                Hi there, how can I help?       1   \n",
       "203635     How are you ?                Hi there, how can I help?       1   \n",
       "203636  Is anyone there?  Good to see you, what can i do for you?       1   \n",
       "203637           Hello !               Hello, how can I help you?       1   \n",
       "203638            Hey !                Hello, how can I help you?       1   \n",
       "\n",
       "        bert_kmeans_35clusters  \n",
       "203634                      35  \n",
       "203635                      35  \n",
       "203636                      35  \n",
       "203637                      35  \n",
       "203638                      35  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data = pd.read_csv('question_responce suppl.csv', index_col='Unnamed: 0')\n",
    "extra_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:01<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_suppl = model.encode(extra_data.question.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "      <th>bert_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203634</th>\n",
       "      <td>Hi !</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203635</th>\n",
       "      <td>How are you ?</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203636</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>Good to see you, what can i do for you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.021577213, -0.018705582, -0.01411042, -0.053509828, 0.030905388, 0.029784491, -0.059820678, -0.00020732067, 0.042898137, -0.07180767, -0.051104918, -0.0010535975, 0.055671804, 0.00908685, 0.07265569, -0.10365003, 0.035366632, -0.024876269, 0.073716044, -0.036032062, 0.03644692, -0.0247611, -0.004494427, -0.015267275, -0.025903398, -2.6341771e-05, 0.017026644, 0.041558318, 0.038424302, 0.016280368, -0.006114492, -0.010574463, -0.009085478, -0.093861654, 1.7027671e-06, -0.006396804, 0.05124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203637</th>\n",
       "      <td>Hello !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203638</th>\n",
       "      <td>Hey !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                responce                                 question  langue  \\\n",
       "203634              Hi !                Hi there, how can I help?       1   \n",
       "203635     How are you ?                Hi there, how can I help?       1   \n",
       "203636  Is anyone there?  Good to see you, what can i do for you?       1   \n",
       "203637           Hello !               Hello, how can I help you?       1   \n",
       "203638            Hey !                Hello, how can I help you?       1   \n",
       "\n",
       "        bert_kmeans_35clusters  \\\n",
       "203634                      35   \n",
       "203635                      35   \n",
       "203636                      35   \n",
       "203637                      35   \n",
       "203638                      35   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               bert_vectors  \n",
       "203634  [0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...  \n",
       "203635  [0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...  \n",
       "203636  [0.021577213, -0.018705582, -0.01411042, -0.053509828, 0.030905388, 0.029784491, -0.059820678, -0.00020732067, 0.042898137, -0.07180767, -0.051104918, -0.0010535975, 0.055671804, 0.00908685, 0.07265569, -0.10365003, 0.035366632, -0.024876269, 0.073716044, -0.036032062, 0.03644692, -0.0247611, -0.004494427, -0.015267275, -0.025903398, -2.6341771e-05, 0.017026644, 0.041558318, 0.038424302, 0.016280368, -0.006114492, -0.010574463, -0.009085478, -0.093861654, 1.7027671e-06, -0.006396804, 0.05124...  \n",
       "203637  [0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...  \n",
       "203638  [0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data['bert_vectors'] = list(embeddings_suppl)\n",
    "extra_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203634</th>\n",
       "      <td>Hi !</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203635</th>\n",
       "      <td>How are you ?</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203636</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>Good to see you, what can i do for you?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.021577213, -0.018705582, -0.01411042, -0.053509828, 0.030905388, 0.029784491, -0.059820678, -0.00020732067, 0.042898137, -0.07180767, -0.051104918, -0.0010535975, 0.055671804, 0.00908685, 0.07265569, -0.10365003, 0.035366632, -0.024876269, 0.073716044, -0.036032062, 0.03644692, -0.0247611, -0.004494427, -0.015267275, -0.025903398, -2.6341771e-05, 0.017026644, 0.041558318, 0.038424302, 0.016280368, -0.006114492, -0.010574463, -0.009085478, -0.093861654, 1.7027671e-06, -0.006396804, 0.05124...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203637</th>\n",
       "      <td>Hello !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203638</th>\n",
       "      <td>Hey !</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                responce                                 question  langue  \\\n",
       "203634              Hi !                Hi there, how can I help?       1   \n",
       "203635     How are you ?                Hi there, how can I help?       1   \n",
       "203636  Is anyone there?  Good to see you, what can i do for you?       1   \n",
       "203637           Hello !               Hello, how can I help you?       1   \n",
       "203638            Hey !                Hello, how can I help you?       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               bert_vectors  \\\n",
       "203634  [0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...   \n",
       "203635  [0.04144959, -0.005290745, -0.030423084, 0.029895034, 0.008074511, 0.010434585, -0.027699664, 0.017426372, 0.05244801, -0.03729783, 0.013165434, -0.017059041, -0.008158231, 0.029116305, -0.026283376, -0.07493479, 0.010359083, 0.020400124, 0.026299704, -0.04182361, 0.085000426, -0.032904096, -0.010974228, -0.021302132, -0.048616953, 0.00016592571, 0.037767656, -0.005562994, 0.056574438, -0.021134185, 0.084714755, 0.0082029365, -0.02009218, -0.02453469, 2.0431228e-06, -0.04244198, 0.022714876,...   \n",
       "203636  [0.021577213, -0.018705582, -0.01411042, -0.053509828, 0.030905388, 0.029784491, -0.059820678, -0.00020732067, 0.042898137, -0.07180767, -0.051104918, -0.0010535975, 0.055671804, 0.00908685, 0.07265569, -0.10365003, 0.035366632, -0.024876269, 0.073716044, -0.036032062, 0.03644692, -0.0247611, -0.004494427, -0.015267275, -0.025903398, -2.6341771e-05, 0.017026644, 0.041558318, 0.038424302, 0.016280368, -0.006114492, -0.010574463, -0.009085478, -0.093861654, 1.7027671e-06, -0.006396804, 0.05124...   \n",
       "203637  [0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...   \n",
       "203638  [0.033737067, 0.008721039, -0.02251804, 0.022312328, -0.01629114, 0.037744936, -0.014925007, 0.015415451, 0.030435676, -0.020164436, -0.00037501485, -0.003719237, 0.01762301, 0.01823256, 0.012304791, -0.09590574, 0.035916466, -2.7873266e-06, 0.024452815, -0.018825488, 0.04863945, -0.01178851, -0.057931513, 0.0051610107, -0.03247865, -0.021967556, 0.04761522, -0.014288692, 0.07796605, 0.028216561, 0.057963334, -0.009269945, -0.021161407, -0.03629033, 2.0842788e-06, -0.01609232, 0.014884227, 0...   \n",
       "\n",
       "        bert_kmeans_35clusters  \n",
       "203634                      35  \n",
       "203635                      35  \n",
       "203636                      35  \n",
       "203637                      35  \n",
       "203638                      35  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data = extra_data.reindex(columns=['responce', 'question', 'langue', 'bert_vectors', 'bert_kmeans_35clusters'])\n",
    "extra_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203634    [0.03138885, -0.06662785, -0.014790138, -0.0098734135, 0.0070758075, 0.032561846, 0.017180264, 0.01291915, 0.0777736, -0.0292635, -0.011801189, -0.078211196, 0.02370826, 0.09880414, 0.0013034958, -0.10994926, -0.011055484, 0.02025685, -0.051953066, -0.030425217, 0.015830478, 0.0022211918, 0.008532129, 0.043706037, 0.014239976, -0.03945647, 0.022131328, 0.019933885, -0.027604941, -0.004125864, 0.03592287, -0.0059972517, 0.017576214, -0.03948228, 2.0300715e-06, -0.014348894, -0.042249896, -0.0...\n",
       "Name: bert_vectors, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data.bert_vectors.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603    [-0.042830452, 0.0033645306, -0.00762816, 0.014212549, 0.068372495, 0.060131975, -0.06108707, 0.023269242, -0.054579534, -0.02880549, 0.02580337, 0.04805528, -0.03956774, 0.07162384, 0.025605988, -0.015029941, -0.036010917, 0.07752178, -0.0028177537, -0.013076511, 0.052667517, 0.0026162898, 0.015808508, 0.013886307, -0.03718728, 0.018579228, -0.014673145, 0.074385084, -0.024294289, -0.03180943, -0.009981542, -0.03329466, -0.04653384, -0.021632569, 1.6013689e-06, -0.027044807, -0.077066176, -...\n",
       "Name: bert_vectors, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bert_vectors.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>Thank you for taking the time to do this</td>\n",
       "      <td>My pleasure</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>Many thanks</td>\n",
       "      <td>My pleasure</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>I’m beyond grateful</td>\n",
       "      <td>My pleasure</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>Thanks in advance</td>\n",
       "      <td>My pleasure</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>Cool</td>\n",
       "      <td>Glad to help!</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.055589296, -0.058139134, -0.03731024, -0.0037887574, 0.008390075, -0.008996939, 0.06166941, 0.056593303, -0.006138974, -0.008134768, 0.025192555, 0.011875059, -0.034955774, 0.029669337, 0.0635257, -0.026505617, -0.026293512, 0.010512616, -0.0060949232, -0.05553912, -0.018489147, -0.0031740272, -0.022357805, 0.061089586, -0.02195238, 0.040936634, 0.038012292, -0.011653122, -0.01885198, -0.099481106, 0.064772315, -0.020187233, -0.035024263, 0.07389704, 2.0994157e-06, -0.02848746, -0.023306...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      responce       question  langue  \\\n",
       "1976  Thank you for taking the time to do this    My pleasure       1   \n",
       "1977                               Many thanks    My pleasure       1   \n",
       "1978                       I’m beyond grateful    My pleasure       1   \n",
       "1979                         Thanks in advance    My pleasure       1   \n",
       "1980                                      Cool  Glad to help!       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             bert_vectors  \\\n",
       "1976  [-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...   \n",
       "1977  [-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...   \n",
       "1978  [-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...   \n",
       "1979  [-0.031142078, -0.06967088, 0.0040144166, -0.020677518, 0.016974067, 0.035821293, 0.0481549, -0.036561247, -0.030646395, 0.031542584, -0.014684711, -0.0016794581, 0.009097712, 0.031373393, 0.088422954, -0.028129455, -0.055544626, 0.03327155, -0.03738446, -0.018758316, 0.0063856724, -0.007508528, -0.041673947, -0.023873983, -0.0034726406, -0.010205084, 0.012690611, 0.055861317, -0.03107079, -0.103001095, 0.0117878765, -0.015880428, -0.04760599, -0.03594868, 1.5173846e-06, 0.036769107, 0.00591...   \n",
       "1980  [-0.055589296, -0.058139134, -0.03731024, -0.0037887574, 0.008390075, -0.008996939, 0.06166941, 0.056593303, -0.006138974, -0.008134768, 0.025192555, 0.011875059, -0.034955774, 0.029669337, 0.0635257, -0.026505617, -0.026293512, 0.010512616, -0.0060949232, -0.05553912, -0.018489147, -0.0031740272, -0.022357805, 0.061089586, -0.02195238, 0.040936634, 0.038012292, -0.011653122, -0.01885198, -0.099481106, 0.064772315, -0.020187233, -0.035024263, 0.07389704, 2.0994157e-06, -0.02848746, -0.023306...   \n",
       "\n",
       "      bert_kmeans_35clusters  \n",
       "1976                      41  \n",
       "1977                      41  \n",
       "1978                      41  \n",
       "1979                      41  \n",
       "1980                      41  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full = data.append(extra_data, ignore_index=True)\n",
    "data_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data_full.csv', index_col='Unnamed: 0')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({33: 92, 10: 84, 8: 79, 26: 79, 7: 75, 6: 72, 12: 68, 4: 64, 29: 61, 21: 60, 25: 59, 3: 58, 5: 58, 2: 58, 28: 57, 15: 53, 24: 51, 11: 51, 23: 51, 1: 50, 22: 49, 13: 47, 31: 45, 34: 43, 16: 43, 30: 39, 14: 38, 18: 36, 9: 36, 0: 32, 38: 31, 19: 30, 27: 29, 41: 28, 20: 27, 40: 25, 35: 24, 39: 23, 36: 22, 37: 22, 32: 21, 17: 11})\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((embeddings, embeddings_suppl), axis=0)\n",
    "y = data_full.bert_kmeans_35clusters\n",
    "\n",
    "from collections import Counter\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters are in the same range but are still unbalanced. We should use SMOTE to oversample the low count classes. Typically undersampling/oversampling will be done on train split only,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "# transform the dataset\n",
    "resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "X_train, y_train = resample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 75, 2: 75, 3: 75, 4: 75, 5: 75, 6: 75, 7: 75, 8: 75, 9: 75, 10: 75, 11: 75, 12: 75, 13: 75, 14: 75, 15: 75, 16: 75, 17: 75, 18: 75, 19: 75, 20: 75, 21: 75, 22: 75, 23: 75, 24: 75, 25: 75, 26: 75, 27: 75, 28: 75, 29: 75, 30: 75, 31: 75, 32: 75, 33: 75, 34: 75, 35: 75, 36: 75, 37: 75, 38: 75, 39: 75, 40: 75, 41: 75, 0: 74})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is far more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62        11\n",
      "           1       0.74      0.88      0.80        16\n",
      "           2       0.74      0.82      0.78        17\n",
      "           3       0.94      0.89      0.91        18\n",
      "           4       0.95      0.87      0.91        23\n",
      "           5       0.73      0.89      0.80        18\n",
      "           6       0.83      0.83      0.83        24\n",
      "           7       0.90      0.75      0.82        24\n",
      "           8       0.87      0.72      0.79        18\n",
      "           9       0.90      0.82      0.86        11\n",
      "          10       0.89      0.86      0.87        28\n",
      "          11       0.74      0.82      0.78        17\n",
      "          12       0.82      0.82      0.82        22\n",
      "          13       0.89      1.00      0.94        16\n",
      "          14       0.86      1.00      0.92         6\n",
      "          15       0.71      0.94      0.81        18\n",
      "          16       0.92      1.00      0.96        12\n",
      "          17       0.00      0.00      0.00         5\n",
      "          18       0.50      0.38      0.43         8\n",
      "          19       0.67      0.80      0.73         5\n",
      "          20       0.91      0.83      0.87        12\n",
      "          21       0.75      0.65      0.70        23\n",
      "          22       0.59      0.87      0.70        15\n",
      "          23       0.67      0.86      0.75        14\n",
      "          24       0.85      0.85      0.85        13\n",
      "          25       0.75      0.79      0.77        19\n",
      "          26       0.81      0.85      0.83        20\n",
      "          27       0.71      0.71      0.71         7\n",
      "          28       1.00      0.67      0.80        12\n",
      "          29       0.88      0.88      0.88        16\n",
      "          30       0.77      0.91      0.83        11\n",
      "          31       0.67      0.53      0.59        19\n",
      "          32       0.83      0.83      0.83         6\n",
      "          33       0.81      0.84      0.82        25\n",
      "          34       0.92      0.85      0.88        13\n",
      "          35       1.00      1.00      1.00         5\n",
      "          36       1.00      1.00      1.00        10\n",
      "          37       1.00      1.00      1.00         4\n",
      "          38       1.00      1.00      1.00         8\n",
      "          39       1.00      1.00      1.00         7\n",
      "          40       1.00      1.00      1.00         6\n",
      "          41       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.82       595\n",
      "   macro avg       0.82      0.82      0.81       595\n",
      "weighted avg       0.82      0.82      0.82       595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# a basic baseline \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "log =  SGDClassifier(loss='log', tol=1e-3)\n",
    "log.fit(X_train, y_train)\n",
    "y_pred = log.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "lgb = lgb.LGBMClassifier(n_estimators=100)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_pred = lgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not beating the baseline and you don't have much more time so we will go with the SGDClassifier. Let's tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qan/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value is: 100\n",
      "0.8453781512605042\n",
      "0.844691319068685\n",
      "C value is: 10\n",
      "0.8638655462184874\n",
      "0.8633357623619223\n",
      "C value is: 1.0\n",
      "0.8689075630252101\n",
      "0.8677167649777517\n",
      "C value is: 0.1\n",
      "0.8369747899159664\n",
      "0.8284242727376933\n",
      "C value is: 0.01\n",
      "0.7210084033613445\n",
      "0.7137048995171199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    " \n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "for c in c_values:\n",
    "    log_reg = LogisticRegression(C=c)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    print(f'C value is: {c}')\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.f1_score(y_test, y_pred, average='weighted')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great improvment, we get 4 more points in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        15\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       0.87      0.93      0.90        14\n",
      "           3       0.83      0.90      0.86        21\n",
      "           4       0.95      0.95      0.95        21\n",
      "           5       0.83      1.00      0.91        15\n",
      "           6       0.88      1.00      0.93        21\n",
      "           7       0.86      0.86      0.86        21\n",
      "           8       0.83      0.87      0.85        23\n",
      "           9       0.58      1.00      0.74         7\n",
      "          10       0.91      0.88      0.89        24\n",
      "          11       1.00      0.84      0.91        19\n",
      "          12       0.94      0.94      0.94        18\n",
      "          13       0.89      0.73      0.80        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       0.78      0.88      0.82        16\n",
      "          16       0.93      0.93      0.93        15\n",
      "          17       1.00      0.50      0.67         2\n",
      "          18       0.78      0.88      0.82         8\n",
      "          19       0.90      1.00      0.95         9\n",
      "          20       0.75      0.75      0.75         8\n",
      "          21       0.93      0.78      0.85        18\n",
      "          22       0.79      0.88      0.83        17\n",
      "          23       0.88      0.74      0.80        19\n",
      "          24       0.69      0.90      0.78        10\n",
      "          25       1.00      0.52      0.68        25\n",
      "          26       0.95      0.86      0.90        22\n",
      "          27       0.86      0.75      0.80         8\n",
      "          28       1.00      0.82      0.90        17\n",
      "          29       0.71      0.94      0.81        16\n",
      "          30       0.77      0.91      0.83        11\n",
      "          31       0.75      0.50      0.60        12\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.52      0.65      0.58        17\n",
      "          34       0.85      0.79      0.81        14\n",
      "          35       0.82      1.00      0.90         9\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       1.00      1.00      1.00         5\n",
      "          38       1.00      1.00      1.00        14\n",
      "          39       1.00      1.00      1.00         3\n",
      "          40       1.00      1.00      1.00        10\n",
      "          41       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.87       595\n",
      "   macro avg       0.88      0.87      0.87       595\n",
      "weighted avg       0.88      0.87      0.87       595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving topics and model\n",
    "pickle.dump(log_reg, open('log_reg.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the enhanced chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent_supervised(embedding):\n",
    "    intent = log_reg.predict(embedding.reshape(1, -1))\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = \"@AmericanAir I am a regular client of your company and I was sitted right next to a woman with a huge dog. And guess what ? I am allergic, how could you allow a 40lb dog to travel among all passengers ? Seriously It's ridiculous...\"\n",
    "test2 = \"@AmericanAir You guys are always late, my flight is reschedule for the third time now... I can't believe this is happening to me again... I can afford to be late at work!\"\n",
    "\n",
    "# here the bot can handle the greetings requirement\n",
    "test3 = \"Thanks a lot!\"\n",
    "\n",
    "# cleaning and vectorizing the user input\n",
    "vectorized_query = encode_input(process_input(test3))\n",
    "\n",
    "# predict the intent of it\n",
    "int(predict_intent_supervised(vectorized_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>@116578 She sure is a gorgeous bird!</td>\n",
       "      <td>thanks awesome home</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03658449, -0.08123066, -0.030130992, -0.010984503, 0.028081981, 0.015881067, -0.020576878, -0.024357129, -0.014168167, -0.01813254, 0.0019290089, 0.013634071, 0.010700653, 0.06937116, 0.04151748, -0.045738406, -0.0066349655, 0.012473571, -0.04985314, -0.07387135, 0.004229141, 0.02816471, -0.02811721, -0.008572966, 0.012062337, 0.025013698, -0.011850557, -0.007837197, -0.074380845, -0.049686246, 0.0035946376, 0.0028954898, -0.03573129, 0.050599713, 1.7436703e-06, -0.041718166, 0.017126385...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                responce             question  langue  \\\n",
       "37  @116578 She sure is a gorgeous bird!  thanks awesome home       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           bert_vectors  \\\n",
       "37  [-0.03658449, -0.08123066, -0.030130992, -0.010984503, 0.028081981, 0.015881067, -0.020576878, -0.024357129, -0.014168167, -0.01813254, 0.0019290089, 0.013634071, 0.010700653, 0.06937116, 0.04151748, -0.045738406, -0.0066349655, 0.012473571, -0.04985314, -0.07387135, 0.004229141, 0.02816471, -0.02811721, -0.008572966, 0.012062337, 0.025013698, -0.011850557, -0.007837197, -0.074380845, -0.049686246, 0.0035946376, 0.0028954898, -0.03573129, 0.050599713, 1.7436703e-06, -0.041718166, 0.017126385...   \n",
       "\n",
       "    bert_kmeans_35clusters  \n",
       "37                      14  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_intent = data_full[data_full['bert_kmeans_35clusters'] == int(predict_intent_supervised(vectorized_query))]\n",
    "subset_intent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13713/1443037527.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_vectors</th>\n",
       "      <th>bert_kmeans_35clusters</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>@136905 Anytime!</td>\n",
       "      <td>thank : )</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.077998795, -0.036685426, -0.03862183, 0.058474313, 0.03804565, 0.060705826, 0.067425944, 0.0967279, -0.035556104, -0.028174294, 0.0015548698, -0.008486141, -0.018125819, 0.03532436, 0.055639505, -0.022637634, 0.00039623783, 0.02692683, -0.03127684, -0.035545155, -0.040714603, 0.010258949, -0.010782627, -0.060221083, 0.051208816, -0.058484025, -0.01270649, -0.01349437, -0.07605338, -0.03730306, -0.03407721, 0.022780878, -0.034736235, 0.0012062243, 1.6541138e-06, 0.009716648, 0.0017264818,...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.576393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>@116578 She sure is a gorgeous bird!</td>\n",
       "      <td>thanks awesome home</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03658449, -0.08123066, -0.030130992, -0.010984503, 0.028081981, 0.015881067, -0.020576878, -0.024357129, -0.014168167, -0.01813254, 0.0019290089, 0.013634071, 0.010700653, 0.06937116, 0.04151748, -0.045738406, -0.0066349655, 0.012473571, -0.04985314, -0.07387135, 0.004229141, 0.02816471, -0.02811721, -0.008572966, 0.012062337, 0.025013698, -0.011850557, -0.007837197, -0.074380845, -0.049686246, 0.0035946376, 0.0028954898, -0.03573129, 0.050599713, 1.7436703e-06, -0.041718166, 0.017126385...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.437329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>@159675 Well then, cheers to you and cheers to us for the best ride through the skies!</td>\n",
       "      <td>thank !</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.05326786, 0.00461571, -0.053443883, 0.07045055, 0.02875867, 0.03576059, 0.019956155, 0.022390898, -0.029715814, 0.017549854, -0.015022508, 0.01680698, -0.07716112, 0.022685306, 0.019174008, -0.016846929, 0.027790308, 0.026650496, -0.007056168, -0.018394012, -0.013388055, 0.026773663, -0.04350769, -0.038571324, 0.0283273, -0.008800344, 0.01929907, 0.03904838, -0.05690575, -0.12216156, 0.018979212, -0.026078876, -0.045588028, 0.035886448, 2.3076432e-06, 0.003413475, 0.031840004, -0.1002012...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.359340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>@144996 You're welcome, Vishal!</td>\n",
       "      <td>thank !</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.05326786, 0.00461571, -0.053443883, 0.07045055, 0.02875867, 0.03576059, 0.019956155, 0.022390898, -0.029715814, 0.017549854, -0.015022508, 0.01680698, -0.07716112, 0.022685306, 0.019174008, -0.016846929, 0.027790308, 0.026650496, -0.007056168, -0.018394012, -0.013388055, 0.026773663, -0.04350769, -0.038571324, 0.0283273, -0.008800344, 0.01929907, 0.03904838, -0.05690575, -0.12216156, 0.018979212, -0.026078876, -0.045588028, 0.035886448, 2.3076432e-06, 0.003413475, 0.031840004, -0.1002012...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.359340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>@153486 Great news, we're so glad to hear that. Have a wonderful rest of the weekend.</td>\n",
       "      <td>made ! thanks much !</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0024258976, -0.033987597, -0.03466017, 0.03367185, 0.02498175, -0.005486265, 0.017503593, 0.0026983055, 0.005791505, 0.06516815, 0.028127586, 0.044665206, -0.03818727, 0.06818986, 0.060075182, -0.0036465444, 0.01801749, 0.015145703, -0.012489212, 0.0029586763, -0.0082673, 0.04904098, -0.011543028, -0.0043557147, -0.022267234, 0.04856372, 0.00026809005, -0.010794449, -0.019360652, -0.058849886, 0.024968987, -0.026097722, -0.063024975, 0.047019254, 1.4333441e-06, -0.03381645, 0.005257447, -...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.352162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    responce  \\\n",
       "553                                                                         @136905 Anytime!   \n",
       "37                                                      @116578 She sure is a gorgeous bird!   \n",
       "1326  @159675 Well then, cheers to you and cheers to us for the best ride through the skies!   \n",
       "826                                                          @144996 You're welcome, Vishal!   \n",
       "991    @153486 Great news, we're so glad to hear that. Have a wonderful rest of the weekend.   \n",
       "\n",
       "                  question  langue  \\\n",
       "553              thank : )       1   \n",
       "37     thanks awesome home       1   \n",
       "1326               thank !       1   \n",
       "826                thank !       1   \n",
       "991   made ! thanks much !       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             bert_vectors  \\\n",
       "553   [-0.077998795, -0.036685426, -0.03862183, 0.058474313, 0.03804565, 0.060705826, 0.067425944, 0.0967279, -0.035556104, -0.028174294, 0.0015548698, -0.008486141, -0.018125819, 0.03532436, 0.055639505, -0.022637634, 0.00039623783, 0.02692683, -0.03127684, -0.035545155, -0.040714603, 0.010258949, -0.010782627, -0.060221083, 0.051208816, -0.058484025, -0.01270649, -0.01349437, -0.07605338, -0.03730306, -0.03407721, 0.022780878, -0.034736235, 0.0012062243, 1.6541138e-06, 0.009716648, 0.0017264818,...   \n",
       "37    [-0.03658449, -0.08123066, -0.030130992, -0.010984503, 0.028081981, 0.015881067, -0.020576878, -0.024357129, -0.014168167, -0.01813254, 0.0019290089, 0.013634071, 0.010700653, 0.06937116, 0.04151748, -0.045738406, -0.0066349655, 0.012473571, -0.04985314, -0.07387135, 0.004229141, 0.02816471, -0.02811721, -0.008572966, 0.012062337, 0.025013698, -0.011850557, -0.007837197, -0.074380845, -0.049686246, 0.0035946376, 0.0028954898, -0.03573129, 0.050599713, 1.7436703e-06, -0.041718166, 0.017126385...   \n",
       "1326  [-0.05326786, 0.00461571, -0.053443883, 0.07045055, 0.02875867, 0.03576059, 0.019956155, 0.022390898, -0.029715814, 0.017549854, -0.015022508, 0.01680698, -0.07716112, 0.022685306, 0.019174008, -0.016846929, 0.027790308, 0.026650496, -0.007056168, -0.018394012, -0.013388055, 0.026773663, -0.04350769, -0.038571324, 0.0283273, -0.008800344, 0.01929907, 0.03904838, -0.05690575, -0.12216156, 0.018979212, -0.026078876, -0.045588028, 0.035886448, 2.3076432e-06, 0.003413475, 0.031840004, -0.1002012...   \n",
       "826   [-0.05326786, 0.00461571, -0.053443883, 0.07045055, 0.02875867, 0.03576059, 0.019956155, 0.022390898, -0.029715814, 0.017549854, -0.015022508, 0.01680698, -0.07716112, 0.022685306, 0.019174008, -0.016846929, 0.027790308, 0.026650496, -0.007056168, -0.018394012, -0.013388055, 0.026773663, -0.04350769, -0.038571324, 0.0283273, -0.008800344, 0.01929907, 0.03904838, -0.05690575, -0.12216156, 0.018979212, -0.026078876, -0.045588028, 0.035886448, 2.3076432e-06, 0.003413475, 0.031840004, -0.1002012...   \n",
       "991   [0.0024258976, -0.033987597, -0.03466017, 0.03367185, 0.02498175, -0.005486265, 0.017503593, 0.0026983055, 0.005791505, 0.06516815, 0.028127586, 0.044665206, -0.03818727, 0.06818986, 0.060075182, -0.0036465444, 0.01801749, 0.015145703, -0.012489212, 0.0029586763, -0.0082673, 0.04904098, -0.011543028, -0.0043557147, -0.022267234, 0.04856372, 0.00026809005, -0.010794449, -0.019360652, -0.058849886, 0.024968987, -0.026097722, -0.063024975, 0.047019254, 1.4333441e-06, -0.03381645, 0.005257447, -...   \n",
       "\n",
       "      bert_kmeans_35clusters  cosine_similarity  \n",
       "553                       14           0.576393  \n",
       "37                        14           0.437329  \n",
       "1326                      14           0.359340  \n",
       "826                       14           0.359340  \n",
       "991                       14           0.352162  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n",
    "subset_intent.sort_values(by=['cosine_similarity'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AmericanAir You guys are always late, my flight is reschedule for the third time now... I can't believe this is happening to me again... I can afford to be late at work!\n",
      "@136905 Anytime!\n",
      "Anytime!\n"
     ]
    }
   ],
   "source": [
    "anwser = subset_intent.sort_values(by=['cosine_similarity'], ascending=False).responce.iat[0]\n",
    "print(test2)\n",
    "print(anwser)\n",
    "print(process_answer(anwser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final chatbot program handling the greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! \n",
      "\n",
      "Thanks a lot!\n",
      "Anytime!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_13713/1237152196.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    \n",
    "    input_str = input()\n",
    "    input_str = str(input_str)\n",
    "\n",
    "    print(\"Processing the input...\")\n",
    "    # cleaning and vectorizing the user input\n",
    "    vectorized_query = encode_input(process_input(input_str))\n",
    "\n",
    "    # get the subset matching the intent\n",
    "    subset_intent = data_full[data_full['bert_kmeans_35clusters'] == int(predict_intent_supervised(vectorized_query))]\n",
    "\n",
    "    # calculate cosine similarity between questions in subset and input\n",
    "    subset_intent['cosine_similarity'] = subset_intent['bert_vectors'].apply(lambda x: 1 - distance.cosine(x, vectorized_query))\n",
    "     # retrieve anwser corresponding to most similar question in subset \n",
    "    anwser = subset_intent.sort_values(by=['cosine_similarity'], ascending=False).responce.iat[0]\n",
    "\n",
    "    print(\"Done!\", '\\n')\n",
    "    print(input_str)\n",
    "    print(process_answer(anwser))\n",
    "    \n",
    "\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
