{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the project\n",
    "\n",
    "There are primarily 3 types of chatbots:\n",
    "\n",
    "* Rule-based: can answer a pre-defined set of statements (questions, chats or requests), and default to a base response in case an unknown statement was provided. This type of chatbots can be useful and accurate when the conversation topic (and potentially questions) are known.  3\n",
    "* AI-based: these chatbots can train from a provided corpus, and can learn to respond to novel questions by generating responces from the provided corpus. This type of chatbots could be more useful in scenarios where discussion topic Is unknown, like in the case with general-purpose chatbots.  \n",
    "* Hybrid: if the statement provided fits the criteria of a pre-defined set of answers, this type of chatbot replies with a pre-defined answer. Otherwise, it can answer in an AI-based method.\n",
    "\n",
    "Here we cant to build a rule-based chatbot focused on customer service.\n",
    "\n",
    "We have unlabeled messages between users and an airline company. It is possible to get some kind of labeling of user intents without doing it by hand ? Intents are the topic of a conversation. We can use the ‚Äúquestions‚Äù column from the training data to extract intents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 15:49:55.797883: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-15 15:49:55.800393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 15:49:55.800402: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/qan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import langid, re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing give her a raise ty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>@AmericanAir Could you have someone on your lax team available to guide me to my gate ASAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. üéÉ #trunkortreat #halloween #2017 #diycostume #parenting @americanair ‚Ä¶ https://t.co/f1nNHQ0iLa https://t.co/lDViDkRdB1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also shouldn‚Äôt have to pay to pass them to my own spouse. You need to change your program.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs and for having great flight attendants on my flight back home!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                                                                                                                              question  \n",
       "0                                                                                                    @AmericanAir Erica on the lax team is amazing give her a raise ty  \n",
       "1                                                                           @AmericanAir Could you have someone on your lax team available to guide me to my gate ASAP  \n",
       "2  Ben Tennyson and an American Airlines pilot. üéÉ #trunkortreat #halloween #2017 #diycostume #parenting @americanair ‚Ä¶ https://t.co/f1nNHQ0iLa https://t.co/lDViDkRdB1  \n",
       "3                                 @AmericanAir Right, but I earned those. I also shouldn‚Äôt have to pay to pass them to my own spouse. You need to change your program.  \n",
       "4                                                         Thank you, @AmericanAir for playing #ThisIsUs and for having great flight attendants on my flight back home!  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('question_responce.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing steps \n",
    "\n",
    "Those messages are really messy and requires a lot of specific preprocessing : removing hastags, url, @ symbols and more. The preprocessing pipeline is part of the optimization process and different kind of processing will work best with different kind of algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove url  \n",
    "def clean_url(df):\n",
    "    tag_url= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    df=tag_url.sub(r'',df)\n",
    "    return df\n",
    "\n",
    "#Remove html link \n",
    "def clean_html(df):\n",
    "    tag_html=re.compile(r'<.*?>')\n",
    "    df=tag_html.sub(r'',df)\n",
    "    return df\n",
    "\n",
    "#Remove all the most recent emojis\n",
    "def remove_emoji(df):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"   \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "                               \n",
    "    return emoji_pattern.sub(r'', df)\n",
    "\n",
    "\n",
    "def get_english(df):\n",
    "  ''' Return True if the sentence is in english\n",
    "      False otherwise'''\n",
    "  return 1 if langid.classify(df)[0] == 'en' else 0\n",
    "\n",
    "\n",
    "def remove_stops(df):\n",
    "    custom_stopwords = set(stopwords.words(\"english\") + ['aa', 'lax', 'flight', 'flying', 'plane', 'flights', 'fly', 'american', 'airlines', 'american_airlines'])\n",
    "    return ' '.join([t for t in word_tokenize(df) if not t in custom_stopwords])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1852, 4)\n",
      "(1818, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>Erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>Could someone team available guide gate ASAP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>Ben Tennyson American Airlines pilot . ‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>Right , I earned . I also ‚Äô pay pass spouse . You need change program .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>Thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                                  question  \\\n",
       "0                                         Erica team amazing give raise ty   \n",
       "1                             Could someone team available guide gate ASAP   \n",
       "2                                 Ben Tennyson American Airlines pilot . ‚Ä¶   \n",
       "3  Right , I earned . I also ‚Äô pay pass spouse . You need change program .   \n",
       "4                             Thank , playing great attendants back home !   \n",
       "\n",
       "   langue  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocess (the order is very important here)\n",
    "data['question']=data['question'].apply(lambda x: re.sub('@[\\w]+','',x)) #delate @\n",
    "data['question']=data['question'].apply(lambda x: re.sub('#[^\\s]+','',x)) #delate hastag\n",
    "data['question']=data['question'].apply(lambda x: clean_url(x))\n",
    "data['question']=data['question'].apply(lambda x: clean_html(x))\n",
    "data['question']=data['question'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# marks english questions with 1 else 0\n",
    "data['langue']=data['question'].apply(lambda x: get_english(x)) \n",
    "print(data.shape)\n",
    "data = data.query('langue == 1') # Remove non english sentences\n",
    "print(data.shape)\n",
    "\n",
    "data['question']=data['question'].apply(lambda x: remove_stops(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "The first step of our work is to extract intents from the dataset. To do so we have a lot of algorithms at our disposal. Basically we want to perform some kind of embedding on the questions and then perform a clustering of the embedded documents. Each cluster will represent an intent.\n",
    "\n",
    "## Using LDA\n",
    "\n",
    "Many algorithms can be used to perform topic modeling, but one very common one is Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic model that assumes that each document is made up of a distribution of a fixed number of topics and each topic is made up of a distribution of words. Coherence is one way to assess the quality of the learned topics by measuring how similar the words are in each topic, and a higher coherence score is better. \n",
    "\n",
    "<br>\n",
    "\n",
    "LDA need extra processing steps like lemmatization. Also since it's a probabilistic model, it's interesting to bring some context with the use of bigrams or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 [erica, team, amazing, give, raise, ty]\n",
       "1                    [could, someone, team, available, guide, gate, asap]\n",
       "2                              [ben, tennyson, american, airlines, pilot]\n",
       "3    [right, earned, also, pay, pass, spouse, you, need, change, program]\n",
       "4                         [thank, playing, great, attendants, back, home]\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "data['question'] = [gensim.utils.simple_preprocess(str(sentence), deacc=True) for sentence in data['question']]\n",
    "data['question'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['might', 'good', 'idea', 'working', 'intercoms', 'flights', 'flight', 'attendant', 'literally', 'sounds', 'like', 'adults', 'talking', 'charlie', 'brown']\n"
     ]
    }
   ],
   "source": [
    "# build bigram model\n",
    "bigram = gensim.models.Phrases(data['question'].tolist(), min_count=3, threshold=50)\n",
    "# we get sentences with the addition of bigrams\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "print(bigram_mod[data['question'].tolist()[55]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "      <td>1</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "      <td>1</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>[ben, tennyson, american, airlines, pilot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[ben, tennyson, american_airlines, pilot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "      <td>1</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "      <td>1</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                               question  \\\n",
       "0                               [erica, team, amazing, give, raise, ty]   \n",
       "1                  [could, someone, team, available, guide, gate, asap]   \n",
       "2                            [ben, tennyson, american, airlines, pilot]   \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]   \n",
       "4                       [thank, playing, great, attendants, back, home]   \n",
       "\n",
       "   langue  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "\n",
       "                                                                bigrams  \n",
       "0                               [erica, team, amazing, give, raise, ty]  \n",
       "1                  [could, someone, team, available, guide, gate, asap]  \n",
       "2                             [ben, tennyson, american_airlines, pilot]  \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]  \n",
       "4                       [thank, playing, great, attendants, back, home]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function to apply the bigram model\n",
    "def get_bigrams(texts):\n",
    "    return bigram_mod[texts]\n",
    "\n",
    "data['bigrams']=data['question'].apply(lambda x: get_bigrams(x))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "      <td>1</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "      <td>[amaze, give, raise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "      <td>1</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "      <td>[team, available, gate, asap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>[ben, tennyson, american, airlines, pilot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[ben, tennyson, american_airlines, pilot]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "      <td>1</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "      <td>[right, earn, also, pay, pass, spouse, need, change, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "      <td>1</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "      <td>[thank, play, great, attendant, back, home]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                               question  \\\n",
       "0                               [erica, team, amazing, give, raise, ty]   \n",
       "1                  [could, someone, team, available, guide, gate, asap]   \n",
       "2                            [ben, tennyson, american, airlines, pilot]   \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]   \n",
       "4                       [thank, playing, great, attendants, back, home]   \n",
       "\n",
       "   langue  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "\n",
       "                                                                bigrams  \\\n",
       "0                               [erica, team, amazing, give, raise, ty]   \n",
       "1                  [could, someone, team, available, guide, gate, asap]   \n",
       "2                             [ben, tennyson, american_airlines, pilot]   \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]   \n",
       "4                       [thank, playing, great, attendants, back, home]   \n",
       "\n",
       "                                                           lemma  \n",
       "0                                           [amaze, give, raise]  \n",
       "1                                  [team, available, gate, asap]  \n",
       "2                                                             []  \n",
       "3  [right, earn, also, pay, pass, spouse, need, change, program]  \n",
       "4                    [thank, play, great, attendant, back, home]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy allow to lemmatize data and filter them by postag in the same time\n",
    "def lemmat_filter(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    doc = nlp(\" \".join(texts)) \n",
    "    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "\n",
    "data['lemma']=data['question'].apply(lambda x: lemmat_filter(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to build the LDA model : \n",
    "- the dictionnary: each word is mapped to a unique id,\n",
    "- the corpus: then we map this id to the frequency of the word for each word in each document, \n",
    "- and finaly we need to choose the number of topics: this is mainly based on the business knowledge or through experience.\n",
    "\n",
    "Alpha and eta are hyperparameters that affect sparsity of the topics. Defaults for both: 1.0/num_topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# building dictionary\n",
    "id2word = corpora.Dictionary(data['lemma'].tolist())\n",
    "\n",
    "# building corpus (frequency of each term in each document)\n",
    "corpus = [id2word.doc2bow(text) for text in data['lemma'].tolist()]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data['lemma'].tolist(), dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "coherence = {}\n",
    "for k in range(5, 30):\n",
    "    coherence[k] = compute_coherence_values(corpus, id2word, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.24921011439616575,\n",
       " 6: 0.2724088159328727,\n",
       " 7: 0.2710172877628945,\n",
       " 8: 0.27561670459239507,\n",
       " 9: 0.29550373614588177,\n",
       " 10: 0.27541733847669303,\n",
       " 11: 0.2864434132179648,\n",
       " 12: 0.2987146642523785,\n",
       " 13: 0.31935762082917285,\n",
       " 14: 0.32114471060228267,\n",
       " 15: 0.28852938591213084,\n",
       " 16: 0.2895129575054921,\n",
       " 17: 0.3183853580912774,\n",
       " 18: 0.30910411206646615,\n",
       " 19: 0.34509347543206004,\n",
       " 20: 0.34238786654915676,\n",
       " 21: 0.3699414418619139,\n",
       " 22: 0.37086236986870486,\n",
       " 23: 0.3406043907035742,\n",
       " 24: 0.3372781413150549,\n",
       " 25: 0.355438842416212,\n",
       " 26: 0.3484660137226754,\n",
       " 27: 0.36658839285629863,\n",
       " 28: 0.35173095331870174,\n",
       " 29: 0.36274954204026716}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the highest coherence score for 22 topics. We retrain a model based on this finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=22, random_state=42,\n",
    "                        update_every=1, chunksize=128, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10,\n",
      "  '0.098*\"pilot\" + 0.089*\"fix\" + 0.084*\"board\" + 0.068*\"feel\" + 0.028*\"speak\" '\n",
      "  '+ 0.025*\"world\" + 0.023*\"computer\" + 0.022*\"hrs\" + 0.014*\"again\" + '\n",
      "  '0.013*\"previous\"'),\n",
      " (13,\n",
      "  '0.200*\"wait\" + 0.096*\"minute\" + 0.092*\"year\" + 0.073*\"almost\" + '\n",
      "  '0.057*\"team\" + 0.055*\"ago\" + 0.015*\"medium\" + 0.015*\"social\" + '\n",
      "  '0.009*\"feedback\" + 0.004*\"compensate\"'),\n",
      " (15,\n",
      "  '0.192*\"amp\" + 0.077*\"crew\" + 0.074*\"awful\" + 0.055*\"problem\" + '\n",
      "  '0.044*\"plane\" + 0.041*\"terminal\" + 0.031*\"whole\" + 0.029*\"though\" + '\n",
      "  '0.025*\"rather\" + 0.025*\"screen\"'),\n",
      " (16,\n",
      "  '0.132*\"want\" + 0.083*\"look\" + 0.063*\"handle\" + 0.055*\"hope\" + '\n",
      "  '0.048*\"answer\" + 0.046*\"video\" + 0.040*\"expect\" + 0.030*\"line\" + '\n",
      "  '0.029*\"sorry\" + 0.024*\"carrier\"'),\n",
      " (9,\n",
      "  '0.128*\"early\" + 0.086*\"able\" + 0.061*\"thing\" + 0.055*\"child\" + 0.046*\"job\" '\n",
      "  '+ 0.040*\"believe\" + 0.035*\"credit\" + 0.029*\"wife\" + 0.027*\"here\" + '\n",
      "  '0.026*\"mom\"'),\n",
      " (19,\n",
      "  '0.196*\"send\" + 0.186*\"day\" + 0.062*\"receive\" + 0.047*\"email\" + '\n",
      "  '0.040*\"update\" + 0.039*\"offer\" + 0.037*\"week\" + 0.024*\"website\" + '\n",
      "  '0.010*\"order\" + 0.008*\"past\"'),\n",
      " (3,\n",
      "  '0.171*\"say\" + 0.155*\"know\" + 0.090*\"claim\" + 0.077*\"passenger\" + 0.066*\"pm\" '\n",
      "  '+ 0.057*\"land\" + 0.042*\"upset\" + 0.024*\"available\" + 0.010*\"charlotte\" + '\n",
      "  '0.003*\"communicate\"'),\n",
      " (20,\n",
      "  '0.171*\"great\" + 0.128*\"upgrade\" + 0.122*\"first\" + 0.116*\"class\" + '\n",
      "  '0.043*\"late\" + 0.029*\"person\" + 0.023*\"cost\" + 0.021*\"club\" + '\n",
      "  '0.014*\"admiral\" + 0.011*\"empty\"'),\n",
      " (11,\n",
      "  '0.168*\"good\" + 0.114*\"never\" + 0.084*\"work\" + 0.080*\"flight\" + '\n",
      "  '0.056*\"happen\" + 0.033*\"lounge\" + 0.028*\"small\" + 0.020*\"there\" + '\n",
      "  '0.019*\"suitcase\" + 0.016*\"literally\"'),\n",
      " (2,\n",
      "  '0.243*\"go\" + 0.140*\"try\" + 0.121*\"airport\" + 0.109*\"book\" + 0.054*\"now\" + '\n",
      "  '0.014*\"wedding\" + 0.007*\"round\" + 0.000*\"need\" + 0.000*\"terrible\" + '\n",
      "  '0.000*\"age\"'),\n",
      " (17,\n",
      "  '0.235*\"gate\" + 0.105*\"experience\" + 0.090*\"agent\" + 0.062*\"response\" + '\n",
      "  '0.037*\"ord\" + 0.037*\"tweet\" + 0.029*\"amazing\" + 0.029*\"helpful\" + '\n",
      "  '0.026*\"platinum\" + 0.023*\"tonight\"'),\n",
      " (7,\n",
      "  '0.215*\"way\" + 0.135*\"change\" + 0.097*\"trip\" + 0.070*\"new\" + 0.044*\"already\" '\n",
      "  '+ 0.039*\"right\" + 0.038*\"program\" + 0.034*\"end\" + 0.034*\"return\" + '\n",
      "  '0.023*\"pass\"'),\n",
      " (14,\n",
      "  '0.110*\"call\" + 0.107*\"dm\" + 0.074*\"give\" + 0.064*\"do\" + '\n",
      "  '0.062*\"international\" + 0.061*\"cancel\" + 0.054*\"number\" + 0.042*\"system\" + '\n",
      "  '0.033*\"phone\" + 0.028*\"staff\"'),\n",
      " (0,\n",
      "  '0.189*\"seat\" + 0.136*\"today\" + 0.094*\"really\" + 0.091*\"charge\" + '\n",
      "  '0.052*\"also\" + 0.037*\"ever\" + 0.024*\"bring\" + 0.024*\"row\" + 0.021*\"put\" + '\n",
      "  '0.019*\"money\"'),\n",
      " (21,\n",
      "  '0.197*\"bag\" + 0.192*\"check\" + 0.104*\"travel\" + 0.076*\"need\" + 0.050*\"let\" + '\n",
      "  '0.032*\"hold\" + 0.032*\"love\" + 0.032*\"nice\" + 0.023*\"baggage\" + '\n",
      "  '0.016*\"flyer\"'),\n",
      " (12,\n",
      "  '0.170*\"hour\" + 0.159*\"delay\" + 0.103*\"still\" + 0.084*\"back\" + 0.053*\"issue\" '\n",
      "  '+ 0.051*\"well\" + 0.045*\"mile\" + 0.044*\"online\" + 0.022*\"due\" + '\n",
      "  '0.019*\"maintenance\"'),\n",
      " (5,\n",
      "  '0.340*\"thank\" + 0.115*\"help\" + 0.081*\"tell\" + 0.068*\"so\" + 0.044*\"ask\" + '\n",
      "  '0.034*\"keep\" + 0.026*\"awesome\" + 0.019*\"guess\" + 0.018*\"name\" + '\n",
      "  '0.016*\"wish\"'),\n",
      " (1,\n",
      "  '0.303*\"get\" + 0.075*\"take\" + 0.070*\"break\" + 0.062*\"see\" + 0.058*\"home\" + '\n",
      "  '0.028*\"family\" + 0.025*\"long\" + 0.023*\"departure\" + 0.022*\"full\" + '\n",
      "  '0.019*\"clt\"'),\n",
      " (4,\n",
      "  '0.145*\"pay\" + 0.128*\"make\" + 0.068*\"ticket\" + 0.066*\"just\" + 0.047*\"price\" '\n",
      "  '+ 0.040*\"refund\" + 0.030*\"free\" + 0.028*\"much\" + 0.027*\"tomorrow\" + '\n",
      "  '0.026*\"pre\"'),\n",
      " (18,\n",
      "  '0.162*\"service\" + 0.114*\"customer\" + 0.114*\"aa\" + 0.090*\"bad\" + 0.051*\"use\" '\n",
      "  '+ 0.045*\"airline\" + 0.043*\"even\" + 0.034*\"leave\" + 0.026*\"boarding\" + '\n",
      "  '0.024*\"switch\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics(num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hour', 0.17007388),\n",
       " ('delay', 0.15917908),\n",
       " ('still', 0.10342963),\n",
       " ('back', 0.0841183),\n",
       " ('issue', 0.05319778),\n",
       " ('well', 0.050760407),\n",
       " ('mile', 0.045235723),\n",
       " ('online', 0.044158585),\n",
       " ('due', 0.022405216),\n",
       " ('maintenance', 0.018532539)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(12, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 0.055338487)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.get_term_topics('team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_max(doc):\n",
    "        idx,l = zip(*doc)\n",
    "        return idx[np.argmax(l)]\n",
    "\n",
    "data['lda_topic'] = [get_max(doc) for doc in lda_model.get_document_topics(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       14\n",
       "1        3\n",
       "2       18\n",
       "3        7\n",
       "4       12\n",
       "        ..\n",
       "1847     9\n",
       "1848     4\n",
       "1849     5\n",
       "1850    14\n",
       "1851     0\n",
       "Name: lda_topic, Length: 1818, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lda_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to cluster our dataset of questions based on the topics extracted with the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Bert Embedding and Kmeans / HDBSCAN clustering \n",
    "\n",
    "Clustering is another very common approach to unsupervised learning problems. We need to encode text data before clustering them. Here we have several alternatives like document embeddings using Doc2vec or transformer based embedding, leveraging the Bert algorithm.\n",
    "\n",
    "Here, we are using Distilbert as it gives a nice balance between speed and performance. With a transformer, there is no real need to lemmatize the data. The model has enough parameters to learn the differences between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:13<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "data['question'] = data['question'].apply(lambda x: \" \".join(x))\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(data.question.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that documents with similar topics are clustered together such that we can find the intents within these clusters.\n",
    "<br> \n",
    "Before doing so, we first need to lower the dimensionality of the embeddings. Indeed, many clustering algorithms handle high dimensionality poorly. After this, we can cluster the documents with HDBSCAN. HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With UMAP + HDBSCAN \n",
    "\n",
    "The n_neighbors parameter controls how UMAP balances local versus global structure in the data. This parameter controls the size of the neighborhood UMAP looks to learn the manifold structure, lower values of n_neighbors will focus more on local structure more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "def get_clusters(embeddings,\n",
    "                      n_neighbors,\n",
    "                      n_components, \n",
    "                      min_cluster_size):\n",
    "                          \n",
    "    umap_emb = (umap.UMAP(n_neighbors=n_neighbors, n_components=n_components, \n",
    "                                metric='cosine', random_state=1)\n",
    "                            .fit_transform(embeddings))\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, metric='euclidean', \n",
    "                               cluster_selection_method='eom').fit(umap_emb)\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweak parameters\n",
    "clusters = get_clusters(embeddings, 10, 15, 16)\n",
    "data[\"bert_dbscan_clusters\"] = clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1     785\n",
       " 1     250\n",
       " 2     125\n",
       " 12    120\n",
       " 6     101\n",
       " 11     89\n",
       " 5      87\n",
       " 10     73\n",
       " 8      40\n",
       " 3      40\n",
       " 9      38\n",
       " 4      29\n",
       " 7      22\n",
       " 0      19\n",
       "Name: bert_dbscan_clusters, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bert_dbscan_clusters.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters don't seem really balanced no matter the parameters we provide. Let's try another clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -1222.0321044921875\n",
      "Silhouette scores: 0.02217666432261467\n",
      "Inertia: -1215.2869873046875\n",
      "Silhouette scores: 0.021896228194236755\n",
      "Inertia: -1208.4901123046875\n",
      "Silhouette scores: 0.023091571405529976\n",
      "Inertia: -1203.1983642578125\n",
      "Silhouette scores: 0.023259736597537994\n",
      "Inertia: -1198.428955078125\n",
      "Silhouette scores: 0.02425963431596756\n",
      "Inertia: -1194.5614013671875\n",
      "Silhouette scores: 0.02488245628774166\n",
      "Inertia: -1193.055419921875\n",
      "Silhouette scores: 0.024535218253731728\n",
      "Inertia: -1184.2222900390625\n",
      "Silhouette scores: 0.023370714858174324\n",
      "Inertia: -1181.4317626953125\n",
      "Silhouette scores: 0.02176203764975071\n",
      "Inertia: -1178.7816162109375\n",
      "Silhouette scores: 0.02049592137336731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69431e0cd0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "losses = []\n",
    "silhouette_scores = []\n",
    "for cluster in range(15,25):\n",
    "    kmeans = KMeans(n_clusters = cluster, random_state=1)\n",
    "    kmeans.fit(embeddings)\n",
    "    print(f'Inertia: {kmeans.score(embeddings)}')\n",
    "    losses.append(kmeans.score(embeddings))\n",
    "    print(f'Silhouette scores: {silhouette_score(embeddings, kmeans.labels_)}')\n",
    "    silhouette_scores.append(silhouette_score(embeddings, kmeans.labels_))\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=20, random_state=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 20, random_state=1)\n",
    "kmeans.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bert_kmeans_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>could someone team available guide gate asap</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>ben tennyson american airlines pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>right earned also pay pass spouse you need change program</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>thank playing great attendants back home</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                    question  langue  \\\n",
       "0                           erica team amazing give raise ty       1   \n",
       "1               could someone team available guide gate asap       1   \n",
       "2                       ben tennyson american airlines pilot       1   \n",
       "3  right earned also pay pass spouse you need change program       1   \n",
       "4                   thank playing great attendants back home       1   \n",
       "\n",
       "   bert_kmeans_clusters  \n",
       "0                    17  \n",
       "1                    18  \n",
       "2                    15  \n",
       "3                     4  \n",
       "4                    19  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['bert_kmeans_clusters'] = kmeans.labels_\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     172\n",
       "19    119\n",
       "14    117\n",
       "6     112\n",
       "2     110\n",
       "18    102\n",
       "11     99\n",
       "10     96\n",
       "16     93\n",
       "3      91\n",
       "15     89\n",
       "8      87\n",
       "0      85\n",
       "5      84\n",
       "13     82\n",
       "4      76\n",
       "7      73\n",
       "17     60\n",
       "1      54\n",
       "12     17\n",
       "Name: bert_kmeans_clusters, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bert_kmeans_clusters.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics looks much more balanced with Kmeans. The topic we get are interesting but let's try to find a better approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2Vec to group sentences by similarity\n",
    "\n",
    "\n",
    "This time we will leverage the power of Word2vec to get a high dimensional representation of words. It's an older technique than transformers but it often provides good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# more traditional deep learning algorithms require some extra preprocessing (vs Transformers)\n",
    "data['question']=data['question'].apply(lambda x: simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can choose to specialize the model on our dataset or use a pretrain version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# build vocabulary and train model on a specific corpus\n",
    "# this looks like transfer learning, we specialize on the corpus of interest\n",
    "# model = Word2Vec(sentences=data['question'].tolist(), window=10,\n",
    "#                     vector_size=300, min_count=2, workers=-1) \n",
    "# model.save('w2v.model')\n",
    "\n",
    "# pretrained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our train model, we can encode our dataset. To work on the document level, we average the embedding of each word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44686/3272721093.py:23: RuntimeWarning: Mean of empty slice.\n",
      "  return result.mean(axis=0) # average of word vectors in sentence\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def vectorize(tokenized_sentence):\n",
    "#     result = []\n",
    "#     for token in tokenized_sentence:\n",
    "#         if token in model.wv: \n",
    "#             result.append(model.wv[token]) # get numpy vector of a word\n",
    "#     result = np.asarray(result)\n",
    "            \n",
    "#     return result.mean(axis=0) # average of word vectors in sentence\n",
    "\n",
    "\n",
    "def vectorize(tokenized_sentence):\n",
    "    result = []\n",
    "    for token in tokenized_sentence:\n",
    "        if token in model: \n",
    "            result.append(model[token]) # get numpy vector of a word\n",
    "    result = np.asarray(result)\n",
    "            \n",
    "    return result.mean(axis=0) # average of word vectors in sentence\n",
    "\n",
    "\n",
    "data['vectorized_question'] = data['question'].apply(vectorize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a clustering algorithm with those embeddings. Using similarity matrix (based on cosine similarity) and hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import  hierarchy\n",
    " \n",
    "# hierarchical clustering based on cosine\n",
    "threshold = 0.5\n",
    "cos_sim_matrix = hierarchy.linkage(data['vectorized_question'].values, \"average\", metric=\"cosine\")\n",
    "clusters = hierarchy.fcluster(cos_sim_matrix, threshold, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with Kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 20, random_state=1)\n",
    "kmeans.fit(data['vectorized_question'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"word2vec_cluster\"] = kmeans.labels_\n",
    "data.word2vec_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Doc2Vec\n",
    "\n",
    "Doc2Vec is an improvment in comparison of the simple averaging of word vectors, it will be more granular in extracting the whole context of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# more traditional deep learning algorithms require some extra preprocessing (vs Transformers)\n",
    "data['question']=data['question'].apply(lambda x: \" \".join(simple_preprocess(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag our dataset\n",
    "questions = data.question\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['erica', 'team', 'amazing', 'give', 'raise', 'ty'], tags=['0'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train Doc2vec model \n",
    "\n",
    "Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, which doesn‚Äôt preserve any word order. \n",
    "\n",
    "Here the order seems to be important, so we use Distributed Memory model with the help of the dm=1 argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 10\n",
      "iteration 20\n",
      "iteration 30\n",
      "iteration 40\n",
      "iteration 50\n",
      "iteration 60\n",
      "iteration 70\n",
      "iteration 80\n",
      "iteration 90\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    " \n",
    "\n",
    "model = Doc2Vec(vector_size=200, alpha=.025, min_alpha=.0005, dm=1, min_count=1, workers=14)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch%10 == 0:\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        \n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= .0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:  [-0.04382083 -0.0205706   0.00856228  0.0008121  -0.0028752 ] \n",
      "\n",
      "Most similar doc:  [('1', 0.3483034074306488), ('1442', 0.21426796913146973), ('140', 0.1648252308368683), ('1100', 0.1543259620666504), ('1079', 0.1309909224510193)] \n",
      "\n",
      "Training vector 1:  [-0.1427832  -0.14770156  0.54968774  0.6640227   1.3668473 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"Could you have someone on your lax team available to guide me to my gate ?\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"Inference: \", v1[:5], \"\\n\")\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.dv.most_similar([v1])\n",
    "print(\"Most similar doc: \", similar_doc[:5], \"\\n\")\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(\"Training vector 1: \", model.dv['1'][:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docvecs/dv property of the Doc2Vec model holds all trained vectors for the 'document tags' seen during training. \n",
    "\n",
    "Now we have trained embeddings and it‚Äôs time to cluster it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>embedded_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>Erica team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>Could someone team available guide gate ASAP</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>Ben Tennyson American Airlines pilot . ‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>Right , I earned . I also ‚Äô pay pass spouse . You need change program .</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>Thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                                  question  \\\n",
       "0                                         Erica team amazing give raise ty   \n",
       "1                             Could someone team available guide gate ASAP   \n",
       "2                                 Ben Tennyson American Airlines pilot . ‚Ä¶   \n",
       "3  Right , I earned . I also ‚Äô pay pass spouse . You need change program .   \n",
       "4                             Thank , playing great attendants back home !   \n",
       "\n",
       "   langue  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     embedded_question  \n",
       "0  [[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...  \n",
       "1  [[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...  \n",
       "2  [[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...  \n",
       "3  [[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...  \n",
       "4  [[-0.2704825, -0.19463374, 0.5872861, 0.1377196, 0.9620227, -0.12048506, -0.5185947, 0.6365868, 0.021077493, 0.99285805, -0.62686443, 0.007765181, -0.0787885, -0.060318545, -0.18594258, -0.27315298, -0.6616099, -0.025207745, -0.10899984, -0.5486979, 0.45712033, 0.096093886, 0.55158067, 0.3555808, 0.15915123, 0.0972394, 0.24523453, -0.46944797, -0.5500069, 0.7076424, 0.62199545, 0.31785524, 0.40168226, 0.41589835, 0.5547192, -0.17608769, 0.0649512, 0.0014388892, 0.33420357, -0.58163524, -1.10...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embedded_question']= model.dv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -86499.53125\n",
      "Silhouette scores: -0.08391577005386353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans_35 = KMeans(n_clusters = 35, random_state=1)\n",
    "kmeans_35.fit(model.dv.vectors)\n",
    "print(f'Inertia: {kmeans_35.score(model.dv.vectors)}')\n",
    "print(f'Silhouette scores: {silhouette_score(model.dv.vectors, kmeans_35.labels_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     271\n",
       "31    201\n",
       "10    139\n",
       "30    112\n",
       "15    112\n",
       "12     99\n",
       "4      93\n",
       "26     91\n",
       "16     79\n",
       "33     77\n",
       "25     71\n",
       "2      60\n",
       "3      58\n",
       "9      50\n",
       "13     49\n",
       "8      47\n",
       "14     46\n",
       "0      45\n",
       "24     41\n",
       "20     27\n",
       "11     23\n",
       "22      4\n",
       "7       4\n",
       "1       4\n",
       "27      4\n",
       "34      2\n",
       "32      1\n",
       "17      1\n",
       "29      1\n",
       "28      1\n",
       "19      1\n",
       "23      1\n",
       "18      1\n",
       "6       1\n",
       "21      1\n",
       "Name: doc2vec_kmeans_35clusters, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['doc2vec_kmeans_35clusters'] = kmeans_35.labels_\n",
    "data['doc2vec_kmeans_35clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we encounter cluster imballanced. Bert is still the best option so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top2Vec\n",
    "\n",
    "Top2Vec is a package that automate the whole intents extraction pipeline, it's faster. But we lose some customization options.\n",
    "\n",
    "Steps:\n",
    "- Generate embedding vectors for documents and words.\n",
    "- Perform dimensionality reduction on the vectors using an algorithm such as UMAP.\n",
    "- Cluster the vectors using a clustering algorithm such as HDBSCAN.\n",
    "- Assign topics to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       Erica lax team amazing give raise ty\n",
       "1                           Could someone lax team available guide gate ASAP\n",
       "2                                   Ben Tennyson American Airlines pilot . ‚Ä¶\n",
       "3    Right , I earned . I also ‚Äô pay pass spouse . You need change program .\n",
       "4                               Thank , playing great attendants back home !\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 16:40:26,716 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2022-06-13 16:40:26,757 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-06-13 16:40:29,131 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-06-13 16:40:29,420 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-06-13 16:40:33,678 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-06-13 16:40:33,713 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec\n",
    "\n",
    "model = Top2Vec(data['question'].values, speed=\"deep-learn\", embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>top2vec_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>Erica lax team amazing give raise ty</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>Could someone lax team available guide gate ASAP</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>Ben Tennyson American Airlines pilot . ‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>Right , I earned . I also ‚Äô pay pass spouse . You need change program .</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>Thank , playing great attendants back home !</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                                  question  \\\n",
       "0                                     Erica lax team amazing give raise ty   \n",
       "1                         Could someone lax team available guide gate ASAP   \n",
       "2                                 Ben Tennyson American Airlines pilot . ‚Ä¶   \n",
       "3  Right , I earned . I also ‚Äô pay pass spouse . You need change program .   \n",
       "4                             Thank , playing great attendants back home !   \n",
       "\n",
       "   langue  top2vec_clusters  \n",
       "0       1                15  \n",
       "1       1                13  \n",
       "2       1                 9  \n",
       "3       1                 6  \n",
       "4       1                15  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_topics(doc):\n",
    "    topics_words, word_scores, topic_scores, topic_nums = model.query_topics(doc, 1)\n",
    "    return topic_nums[0]\n",
    "\n",
    "data['top2vec_clusters']=data['question'].apply(lambda x: query_topics(x))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     146\n",
       "1     144\n",
       "3     136\n",
       "0     136\n",
       "5     125\n",
       "4     109\n",
       "6     101\n",
       "7      97\n",
       "8      94\n",
       "12     84\n",
       "9      77\n",
       "15     76\n",
       "10     69\n",
       "13     68\n",
       "11     67\n",
       "16     64\n",
       "17     55\n",
       "14     51\n",
       "19     44\n",
       "18     44\n",
       "20     31\n",
       "Name: top2vec_clusters, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['top2vec_clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are balanced, it seems that we are going to explore the actual intents with LDA, Bert and Top2Vec. We eliminate Doc2vec and Word2vec so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intents Extraction \n",
    "\n",
    "Here we want to understand what our clusters actually mean.\n",
    "\n",
    "An interesting approach for this is to obtain the most common action / object pair from the phrases in each cluster as the cluster label (\"refund / policy\", \"delay / flight\"). Spacy library is amazing for this task because we can extract verbs, ojects, nouns... from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def most_common(lst, n_words):\n",
    "    \n",
    "    counter=collections.Counter(lst)\n",
    "    return counter.most_common(n_words)\n",
    "\n",
    "def extract_labels(list_docs):\n",
    "\n",
    "    verbs = []\n",
    "    dobjs = []\n",
    "    nouns = []\n",
    "    adjs = []\n",
    "    \n",
    "    verb = ''\n",
    "    dobj = ''\n",
    "    noun1 = ''\n",
    "    noun2 = ''\n",
    "\n",
    "    # for each document, append verbs, dobs, nouns, and adjectives to \n",
    "    # running lists for whole cluster\n",
    "    for i in range(len(list_docs)):\n",
    "        doc = nlp(list_docs[i])\n",
    "        for token in doc:\n",
    "            if token.is_stop==False:\n",
    "                if token.dep_ == 'ROOT':\n",
    "                    verbs.append(token.text.lower())\n",
    "                elif token.dep_=='dobj':\n",
    "                    dobjs.append(token.lemma_.lower())\n",
    "                elif token.pos_=='NOUN':\n",
    "                    nouns.append(token.lemma_.lower())     \n",
    "                elif token.pos_=='ADJ':\n",
    "                    adjs.append(token.lemma_.lower())\n",
    "    \n",
    "    # take most common words of each kind\n",
    "    if len(verbs) > 0:\n",
    "        verb = most_common(verbs, 1)[0][0]\n",
    "    if len(dobjs) > 0:\n",
    "        dobj = most_common(dobjs, 1)[0][0]\n",
    "    if len(nouns) > 0:\n",
    "        noun1 = most_common(nouns, 1)[0][0]\n",
    "    if len(set(nouns)) > 1:\n",
    "        noun2 = most_common(nouns, 2)[1][0]\n",
    "    \n",
    "    # concatenate the most common words of each kind\n",
    "    label_words = [verb, dobj]\n",
    "    \n",
    "    for word in [noun1, noun2]:\n",
    "        if word not in label_words:\n",
    "            label_words.append(word)\n",
    "\n",
    "    if '' in label_words:\n",
    "        label_words.remove('')\n",
    "    \n",
    "    label = '_'.join(label_words)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TOP2VEC :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = {}\n",
    "for cluster in data['top2vec_clusters'].unique().tolist():\n",
    "    intents['cluster_' + str(cluster)] = extract_labels(data[data['top2vec_clusters']==cluster]['question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_15': 'thank_job_today',\n",
       " 'cluster_13': 'gate_gate_agent',\n",
       " 'cluster_9': '@_miami_thank_today',\n",
       " 'cluster_6': 'booked_ticket_mile',\n",
       " 'cluster_3': 'rude_service_attendant',\n",
       " 'cluster_19': 'help_app_problem',\n",
       " 'cluster_7': 'thank_response_guy',\n",
       " 'cluster_11': 'dfw_pilot_hour_tomorrow',\n",
       " 'cluster_20': 'able',\n",
       " 'cluster_17': 'upgrade_upgrade_thank',\n",
       " 'cluster_12': 'need_mind_time_today',\n",
       " 'cluster_1': 'check_bag_baggage',\n",
       " 'cluster_16': 'got_meal_amp_food',\n",
       " 'cluster_5': 'want_seat_row',\n",
       " 'cluster_2': 'delayed_delay_hour',\n",
       " 'cluster_8': 'thank_family_holiday_day',\n",
       " 'cluster_10': 'sitting_tarmac_hour_gate',\n",
       " 'cluster_0': 'cancelled_customer_day_pilot',\n",
       " 'cluster_4': 'service_service_customer',\n",
       " 'cluster_14': 'aa_aa_year',\n",
       " 'cluster_18': 'dm_dm_record'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BERT/KMEANS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['question'] = data['question'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "intents = {}\n",
    "for cluster in data['bert_kmeans_clusters'].unique().tolist():\n",
    "    intents['cluster_' + str(cluster)] = extract_labels(data[data['bert_kmeans_clusters']==cluster]['question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_17': 'tell_friend_medium_thank',\n",
       " 'cluster_18': 'send_response_ticket_aa',\n",
       " 'cluster_15': 'flown_pilot_time',\n",
       " 'cluster_4': 'pay_ticket_mile',\n",
       " 'cluster_19': 'thank_attendant_flight_service',\n",
       " 'cluster_5': 'delayed_hour_delay',\n",
       " 'cluster_8': 'called_ticket_day_customer',\n",
       " 'cluster_2': 'paid_seat_class',\n",
       " 'cluster_7': 'thank_thank_dm',\n",
       " 'cluster_16': 'tried_response_upgrade_app',\n",
       " 'cluster_6': 'thank_thank_today',\n",
       " 'cluster_9': 'going_family_time_year',\n",
       " 'cluster_1': 'carry_bag_gate',\n",
       " 'cluster_3': 'closed_home_aa_amp',\n",
       " 'cluster_10': 'cancelled_refund_customer_cancellation',\n",
       " 'cluster_11': 'delayed_gate_hour',\n",
       " 'cluster_13': 'bags_bag_luggage',\n",
       " 'cluster_0': 'understand_service_customer',\n",
       " 'cluster_14': 'dfw_screen_thank_day',\n",
       " 'cluster_12': ''}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>langue</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lda_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind words! #AATeam</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "      <td>1</td>\n",
       "      <td>[erica, team, amazing, give, raise, ty]</td>\n",
       "      <td>[amaze, give, raise]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "      <td>1</td>\n",
       "      <td>[could, someone, team, available, guide, gate, asap]</td>\n",
       "      <td>[team, available, gate, asap]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot in the making! #HappyHalloween</td>\n",
       "      <td>[ben, tennyson, american, airlines, pilot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[ben, tennyson, american_airlines, pilot]</td>\n",
       "      <td>[]</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "      <td>1</td>\n",
       "      <td>[right, earned, also, pay, pass, spouse, you, need, change, program]</td>\n",
       "      <td>[right, earn, also, pay, pass, spouse, need, change, program]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "      <td>1</td>\n",
       "      <td>[thank, playing, great, attendants, back, home]</td>\n",
       "      <td>[thank, play, great, attendant, back, home]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0         603   \n",
       "1         605   \n",
       "2         608   \n",
       "3         612   \n",
       "4         618   \n",
       "\n",
       "                                                                                                                       responce  \\\n",
       "0                                                                  @115904 We'll be sure to pass along your kind words! #AATeam   \n",
       "1  @115904 Our apologies for the delay in responding to you. Have you made it to LAX? Let us know if you still need assistance.   \n",
       "2                                                  @115905 Aww, that's definitely a future pilot in the making! #HappyHalloween   \n",
       "3                                                                                     @115906 We're sorry for your frustration.   \n",
       "4                            @115909 We're glad you got to kick back and enjoy a show while flying! Thanks for your kind words.   \n",
       "\n",
       "                                                               question  \\\n",
       "0                               [erica, team, amazing, give, raise, ty]   \n",
       "1                  [could, someone, team, available, guide, gate, asap]   \n",
       "2                            [ben, tennyson, american, airlines, pilot]   \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]   \n",
       "4                       [thank, playing, great, attendants, back, home]   \n",
       "\n",
       "   langue  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "\n",
       "                                                                bigrams  \\\n",
       "0                               [erica, team, amazing, give, raise, ty]   \n",
       "1                  [could, someone, team, available, guide, gate, asap]   \n",
       "2                             [ben, tennyson, american_airlines, pilot]   \n",
       "3  [right, earned, also, pay, pass, spouse, you, need, change, program]   \n",
       "4                       [thank, playing, great, attendants, back, home]   \n",
       "\n",
       "                                                           lemma  lda_topic  \n",
       "0                                           [amaze, give, raise]         14  \n",
       "1                                  [team, available, gate, asap]          3  \n",
       "2                                                             []         18  \n",
       "3  [right, earn, also, pay, pass, spouse, need, change, program]          7  \n",
       "4                    [thank, play, great, attendant, back, home]         12  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             erica team amazing give raise ty\n",
       "1                 could someone team available guide gate asap\n",
       "2                         ben tennyson american airlines pilot\n",
       "3    right earned also pay pass spouse you need change program\n",
       "4                     thank playing great attendants back home\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question'] = data['question'].apply(lambda x: \" \".join(x))\n",
    "data['question'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = {}\n",
    "for cluster in data['lda_topic'].unique().tolist():\n",
    "    intents['cluster_' + str(cluster)] = extract_labels(data[data['lda_topic']==cluster]['question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_14': 'cancelled_status_dm_record',\n",
       " 'cluster_3': 'know_passenger_pilot_gate',\n",
       " 'cluster_18': 'help_service_customer',\n",
       " 'cluster_7': 'thank_trip_way',\n",
       " 'cluster_12': 'delayed_issue_hour_delay',\n",
       " 'cluster_4': 'thank_ticket_bag',\n",
       " 'cluster_20': 'upgraded_class_club',\n",
       " 'cluster_2': 'going_pm_airport_minute',\n",
       " 'cluster_9': 'able_leg_child_mom',\n",
       " 'cluster_11': 'happened_people_flight_idea',\n",
       " 'cluster_1': 'got_time_home_hour',\n",
       " 'cluster_13': 'waiting_year_minute_medium',\n",
       " 'cluster_15': 'cause_problem_amp',\n",
       " 'cluster_8': 'flown_connection_time_month',\n",
       " 'cluster_10': 'said_time_pilot_minute',\n",
       " 'cluster_21': 'need_bag_baggage',\n",
       " 'cluster_5': 'thank_thank_year',\n",
       " 'cluster_6': 'think_point_people_dog',\n",
       " 'cluster_16': 'want_customer_people_flagship',\n",
       " 'cluster_0': 'bring_seat_today',\n",
       " 'cluster_17': 'dfw_agent_gate',\n",
       " 'cluster_19': 'sent_website_week_day'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the BERT + KMEANS combinaison gave us the best results. We will now further optimize this model and build the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
